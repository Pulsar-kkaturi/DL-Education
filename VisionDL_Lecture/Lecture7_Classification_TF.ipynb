{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pulsar-kkaturi/DL-Education/blob/master/VisionDL_Lecture/Lecture6_Classification_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "vZsRrUPS9N7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Library Import"
      ],
      "metadata": {
        "id": "Yk6piLxYA-V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, matplotlib, csv, shutil, json, random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import skimage\n",
        "from skimage import io as skio\n",
        "from skimage import transform as skit\n",
        "\n",
        "### Tensorflow 2.0 ###\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "\n",
        "# scikit-learn\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YA35BpKU9Sb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인\n",
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices()"
      ],
      "metadata": {
        "id": "N1vs_QvvA473"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ResNet"
      ],
      "metadata": {
        "id": "HeOc-T_69PvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Data Loading\n",
        "* Kaggle Garbage Classification Dataset\n",
        "  - Reference link: https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification"
      ],
      "metadata": {
        "id": "KUI117Ik9Z2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ybj88hl8wFh"
      },
      "outputs": [],
      "source": [
        "# Dataset Download\n",
        "!git clone https://github.com/Pulsar-kkaturi/DL-Education.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 압축 풀기\n",
        "!tar -zxf ./DL-Education/dataset/garbage_cls_2d.tar.gz"
      ],
      "metadata": {
        "id": "BSQBbIeJ9I0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './garbage_cls_2d'\n",
        "label_list = list(sorted(os.listdir(data_path)))\n",
        "print(label_list)"
      ],
      "metadata": {
        "id": "Jr-zay4F9y3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data number check\n",
        "for c in label_list:\n",
        "  cn = 0\n",
        "  cls_path = os.path.join(data_path, c)\n",
        "  for f in os.listdir(cls_path):\n",
        "    cn += 1\n",
        "  print(f'{c} number = {cn}')"
      ],
      "metadata": {
        "id": "gYETh9Gz-clf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visulization\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, c in enumerate(label_list):\n",
        "  cls_path = os.path.join(data_path, c)\n",
        "  sam_path = os.path.join(cls_path, os.listdir(cls_path)[0])\n",
        "  sam_arr = skio.imread(sam_path)\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.title(c)\n",
        "  plt.imshow(sam_arr)"
      ],
      "metadata": {
        "id": "lCMOB4Iy-cKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Data Pre-processing"
      ],
      "metadata": {
        "id": "5hkSIdI0_9d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1. 데이터 변환"
      ],
      "metadata": {
        "id": "RUxLIWR6fGQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding\n",
        "for c in label_list:\n",
        "  onehot = [0]*len(label_list)\n",
        "  onehot[label_list.index(c)] = 1\n",
        "  print(c, label_list.index(c), onehot)"
      ],
      "metadata": {
        "id": "_N5nXgk8Bie7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy array로 데이터 변환\n",
        "data_dic, label_dic = {}, {}\n",
        "for key in label_list:\n",
        "  data_dic[key] = []\n",
        "  label_dic[key] = []\n",
        "\n",
        "for i, c in enumerate(label_list):\n",
        "  cls_path = os.path.join(data_path, c)\n",
        "  for f in sorted(os.listdir(cls_path)):\n",
        "    file_path = os.path.join(cls_path, f)\n",
        "    data_arr = skio.imread(file_path)\n",
        "    onehot = [0]*len(label_list)\n",
        "    onehot[label_list.index(c)] = 1\n",
        "    data_dic[c].append(data_arr)\n",
        "    label_dic[c].append(onehot)"
      ],
      "metadata": {
        "id": "LcuEt-s5_ZnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "for i, k in enumerate(data_dic.keys()):\n",
        "  print(f'{k}: data_number = {len(data_dic[k])}, label_number = {len(label_dic[k])}')\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.title(f'{k}: {label_dic[k][0]}')\n",
        "  plt.imshow(data_dic[k][0])"
      ],
      "metadata": {
        "id": "hkAMfQpRBcnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2. 데이터셋 세팅"
      ],
      "metadata": {
        "id": "tI0mT1wTfL_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Validation-Test settig & resizing (256, 256)\n",
        "img_size = (128, 128)\n",
        "val_num = 50\n",
        "test_num = 5\n",
        "\n",
        "train_x_list, train_y_list = [], []\n",
        "val_x_list, val_y_list = [], []\n",
        "test_x_list, test_y_list = [], []\n",
        "\n",
        "for k in label_list:\n",
        "  vn, tn = 0, 0\n",
        "  for i, arr in enumerate(data_dic[k]):\n",
        "    rs_arr = skit.resize(arr, img_size, anti_aliasing=True)\n",
        "    if tn < test_num:\n",
        "      test_x_list.append(rs_arr)\n",
        "      test_y_list.append(label_dic[k][i])\n",
        "      tn += 1\n",
        "    elif vn < val_num:\n",
        "      val_x_list.append(rs_arr)\n",
        "      val_y_list.append(label_dic[k][i])\n",
        "      vn += 1\n",
        "    else:\n",
        "      train_x_list.append(rs_arr)\n",
        "      train_y_list.append(label_dic[k][i])\n",
        "\n",
        "print(len(train_x_list), len(val_x_list), len(test_x_list))\n",
        "print(len(train_y_list), len(val_y_list), len(test_y_list))"
      ],
      "metadata": {
        "id": "NRrtjqTzE9ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.Random(1814).shuffle(train_x_list)\n",
        "random.Random(1814).shuffle(train_y_list)\n",
        "random.Random(1814).shuffle(val_x_list)\n",
        "random.Random(1814).shuffle(val_y_list)\n",
        "random.Random(1814).shuffle(test_x_list)\n",
        "random.Random(1814).shuffle(test_y_list)\n",
        "\n",
        "train_x = np.array(train_x_list, dtype=np.float32)\n",
        "train_y = np.array(train_y_list, dtype=np.uint8)\n",
        "val_x = np.array(val_x_list, dtype=np.float32)\n",
        "val_y = np.array(val_y_list, dtype=np.uint8)\n",
        "test_x = np.array(test_x_list, dtype=np.float32)\n",
        "test_y = np.array(test_y_list, dtype=np.uint8)\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(val_x.shape, val_y.shape)\n",
        "print(test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "o4bmv7wDFOyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.title(f'{label_list[train_y[i].argmax()]}: {train_y[i]}')\n",
        "  plt.imshow(train_x[i])"
      ],
      "metadata": {
        "id": "A7OttsaTIQDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Model Build"
      ],
      "metadata": {
        "id": "kYp7kbU1hB1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.1. Keras application"
      ],
      "metadata": {
        "id": "-hc4WXm2hMEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5wy-jqUBgfNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.2. ResNet Block"
      ],
      "metadata": {
        "id": "P5ojfRFahTpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock2D:\n",
        "    def conv1_block(self, lr_conv):\n",
        "        # layer blcok\n",
        "        lr_conv = layers.ZeroPadding2D(3)(lr_conv)\n",
        "        lr_conv = layers.Conv2D(64, 7, 2, activation=None,\n",
        "                                    padding='valid', kernel_initializer='he_normal')(lr_conv)\n",
        "        lr_conv = layers.BatchNormalization(axis=-1)(lr_conv)\n",
        "        lr_conv = layers.Activation('relu')(lr_conv)\n",
        "        lr_conv = layers.ZeroPadding2D(1)(lr_conv)\n",
        "        lr_conv = layers.MaxPool2D(3, 2, padding='valid')(lr_conv)\n",
        "        return lr_conv\n",
        "    def res_conv_block(self, lr_io, ker_size, block_num, reg_weight, act_func, mode=None):\n",
        "        for i in range(block_num):\n",
        "            if mode == 'hold':\n",
        "                fstr = 1\n",
        "            else:\n",
        "                fstr = 2\n",
        "            # layer block\n",
        "            if i == 0:\n",
        "                lr_conv1 = layers.Conv2D(ker_size, 1, fstr, padding='same', kernel_initializer='he_normal',\n",
        "                                         kernel_regularizer=regularizers.l2(reg_weight))(lr_io)\n",
        "            else:\n",
        "                lr_conv1 = layers.Conv2D(ker_size, 1, 1, padding='same', kernel_initializer='he_normal',\n",
        "                                         kernel_regularizer=regularizers.l2(reg_weight))(lr_io)\n",
        "            lr_conv1 = layers.BatchNormalization(axis=-1)(lr_conv1)\n",
        "            lr_conv1 = layers.Activation(act_func)(lr_conv1)\n",
        "            lr_conv2 = layers.Conv2D(ker_size, 3, 1, padding='same', kernel_initializer='he_normal',\n",
        "                                         kernel_regularizer=regularizers.l2(reg_weight))(lr_conv1)\n",
        "            lr_conv2 = layers.BatchNormalization(axis=-1)(lr_conv2)\n",
        "            lr_conv2 = layers.Activation(act_func)(lr_conv2)\n",
        "            lr_conv3 = layers.Conv2D(4*ker_size, 1, 1, padding='same', kernel_initializer='he_normal',\n",
        "                                         kernel_regularizer=regularizers.l2(reg_weight))(lr_conv2)\n",
        "            lr_conv3 = layers.BatchNormalization(axis=-1)(lr_conv3)\n",
        "            if i == 0:\n",
        "                lr_conv0 = layers.Conv2D(4*ker_size, 1, fstr, padding='same', kernel_initializer='he_normal',\n",
        "                                         kernel_regularizer=regularizers.l2(reg_weight))(lr_io)\n",
        "                lr_conv0 = layers.BatchNormalization(axis=-1)(lr_conv0)\n",
        "                lr_add = layers.Add()([lr_conv0, lr_conv3])\n",
        "            else:\n",
        "                lr_add = layers.Add()([lr_io, lr_conv3])\n",
        "            lr_io = layers.Activation(act_func)(lr_add)\n",
        "        return lr_io\n",
        "    def gap_block(self, lr_dense, act_func, drop_rate):\n",
        "        lr_dense = layers.GlobalAveragePooling2D()(lr_dense)\n",
        "        lr_dense = layers.Dropout(drop_rate)(lr_dense)\n",
        "        lr_dense = layers.Dense(1000, activation=act_func)(lr_dense)\n",
        "        lr_dense = layers.Dropout(drop_rate)(lr_dense)\n",
        "        return lr_dense\n",
        "\n",
        "\n",
        "rb = ResNetBlock2D()"
      ],
      "metadata": {
        "id": "HBsmOPL7hYqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.3. ResNet50"
      ],
      "metadata": {
        "id": "payw9jy2heKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50_2D(input_size, channel_num, par_dic):\n",
        "    # parameters\n",
        "    reg_weight = par_dic['reg_weight']\n",
        "    act_func = par_dic['act_func']\n",
        "    drop_rate = par_dic['drop_rate']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "    # code block\n",
        "    inputs = layers.Input(shape=(input_size, input_size, channel_num))\n",
        "    conv1 = rb.conv1_block(inputs) # Block1\n",
        "    conv2 = rb.res_conv_block(conv1, 64, 3, reg_weight, act_func, mode='hold') # Block2\n",
        "    conv3 = rb.res_conv_block(conv2, 128, 4, reg_weight, act_func) # Block3\n",
        "    conv4 = rb.res_conv_block(conv3, 256, 6, reg_weight, act_func) # Block4\n",
        "    conv5 = rb.res_conv_block(conv4, 512, 3, reg_weight, act_func) # Block5\n",
        "    dens = rb.gap_block(conv5, act_func, drop_rate)\n",
        "    outputs = layers.Dense(output_count, activation=output_act)(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "YucpuLsFhggn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet101_2D(input_size, channel_num, par_dic):\n",
        "    # parameters\n",
        "    reg_weight = par_dic['reg_weight']\n",
        "    act_func = par_dic['act_func']\n",
        "    drop_rate = par_dic['drop_rate']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "    # code block\n",
        "    inputs = layers.Input(shape=(input_size, input_size, channel_num))\n",
        "    conv1 = rb.conv1_block(inputs) # Blcok1\n",
        "    conv2 = rb.res_conv_block(conv1, 64, 3, reg_weight, act_func, mode='hold') # Block2\n",
        "    conv3 = rb.res_conv_block(conv2, 128, 4, reg_weight, act_func) # Block3\n",
        "    conv4 = rb.res_conv_block(conv3, 256, 23, reg_weight, act_func) # Block4\n",
        "    conv5 = rb.res_conv_block(conv4, 512, 3, reg_weight, act_func) # Block5\n",
        "    dens = rb.gap_block(conv5, act_func, drop_rate)\n",
        "    outputs = layers.Dense(output_count, activation=output_act)(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-5hVq58PkJj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet152_2D(input_size, channel_num, par_dic):\n",
        "    # parameters\n",
        "    reg_weight = par_dic['reg_weight']\n",
        "    act_func = par_dic['act_func']\n",
        "    drop_rate = par_dic['drop_rate']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "    # code block\n",
        "    inputs = layers.Input(shape=(input_size, input_size, channel_num))\n",
        "    conv1 = rb.conv1_block(inputs) # Blcok1\n",
        "    conv2 = rb.res_conv_block(conv1, 64, 3, reg_weight, act_func, mode='hold') # Block2\n",
        "    conv3 = rb.res_conv_block(conv2, 128, 8, reg_weight, act_func) # Block3\n",
        "    conv4 = rb.res_conv_block(conv3, 256, 36, reg_weight, act_func) # Block4\n",
        "    conv5 = rb.res_conv_block(conv4, 512, 3, reg_weight, act_func) # Block5\n",
        "    dens = rb.gap_block(conv5, act_func, drop_rate)\n",
        "    outputs = layers.Dense(output_count, activation=output_act)(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "5rRfD6KBkg9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_param = {'reg_weight': None,\n",
        "               'act_func': 'relu',\n",
        "               'drop_rate': 0.5,\n",
        "               'output_count': 6,\n",
        "               'output_act': 'softmax'}\n",
        "\n",
        "# ResNet 50, 101, 152 중에서 원하는 모델 사용\n",
        "model = ResNet50_2D(128, 3, resnet_param)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3sSUDAFWky_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Train Model"
      ],
      "metadata": {
        "id": "a9NUhQLwoUf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5tcw52ytk8ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
        "                         keras.callbacks.ModelCheckpoint(filepath='resnet_model.h5',\n",
        "                                                         monitor='val_loss', save_best_only=True),\n",
        "                         keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)]\n",
        "\n",
        "history = model.fit(train_x, train_y, epochs=50, batch_size=16,\n",
        "                    validation_data=(val_x, val_y),\n",
        "                    callbacks=callback_list, shuffle=True)"
      ],
      "metadata": {
        "id": "6CSYhCbWoaRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Train Result"
      ],
      "metadata": {
        "id": "Cs0DXHRLtTfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5.1. Loss & Accuracy"
      ],
      "metadata": {
        "id": "KRRDpZ8HtWWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,len(acc)+1)"
      ],
      "metadata": {
        "id": "9QpEt5NstSID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y-L5wdlyta_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5.2. Prediction"
      ],
      "metadata": {
        "id": "x7ZljNP9tqhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.evaluate(test_x, test_y)\n",
        "print('test loss = ', result[0])\n",
        "print('test Accuracy = ', result[1])"
      ],
      "metadata": {
        "id": "ey9vf-swvcWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(test_x)\n",
        "print(len(scores))\n",
        "print(label_list[test_y[0].argmax()], test_y[0])\n",
        "print(label_list[scores[0].argmax()], scores[0])"
      ],
      "metadata": {
        "id": "BNM0yqaKt5Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "  gt = label_list[test_y[i].argmax()]\n",
        "  pd = label_list[scores[i].argmax()]\n",
        "  score = 'True' if gt == pd else 'False'\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.title(f'{score}({pd}/{gt})')\n",
        "  plt.imshow(test_x[i])"
      ],
      "metadata": {
        "id": "7NmTeZUWuNOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5.3. Confusion Matrix"
      ],
      "metadata": {
        "id": "pJAPxAx5vvdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gt_list = [label_list[test_y[i].argmax()] for i in range(test_y.shape[0])]\n",
        "pd_list = [label_list[scores[i].argmax()] for i in range(len(scores))]\n",
        "\n",
        "print(gt_list)\n",
        "print(pd_list)\n",
        "\n",
        "conf = confusion_matrix(gt_list, pd_list, labels=label_list)\n",
        "print(conf)"
      ],
      "metadata": {
        "id": "s-qjn-YPu5-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.heatmap(conf, annot = True, cmap=\"coolwarm\", vmax = 5,\n",
        "                 annot_kws={\"fontsize\":20}, center=3, cbar=True,\n",
        "                 xticklabels=label_list, yticklabels=label_list)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels',fontsize=20)\n",
        "ax.set_ylabel('True labels',fontsize=20)\n",
        "ax.set_title('Confusion Matrix', fontsize=30)\n",
        "ax.xaxis.set_ticklabels(label_list, fontsize=10)\n",
        "ax.yaxis.set_ticklabels(label_list, fontsize=10)"
      ],
      "metadata": {
        "id": "dt6oUKyPwOo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5.4. Case Analysis"
      ],
      "metadata": {
        "id": "8TOZ__0qyPqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# False Case\n",
        "plt.figure(figsize=(10,10))\n",
        "pn = 0\n",
        "for i in range(len(scores)):\n",
        "  gt = label_list[test_y[i].argmax()]\n",
        "  pd = label_list[scores[i].argmax()]\n",
        "  score = 'True' if gt == pd else 'False'\n",
        "  if score == 'False' and pn < 9:\n",
        "    pn += 1\n",
        "    plt.subplot(3,3,pn)\n",
        "    plt.title(f'{score}({pd}/{gt})')\n",
        "    plt.imshow(test_x[i])"
      ],
      "metadata": {
        "id": "6nvEtEaxybTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True Case\n",
        "plt.figure(figsize=(10,10))\n",
        "pn = 0\n",
        "for i in range(len(scores)):\n",
        "  gt = label_list[test_y[i].argmax()]\n",
        "  pd = label_list[scores[i].argmax()]\n",
        "  score = 'True' if gt == pd else 'False'\n",
        "  if score == 'True' and pn < 9:\n",
        "    pn += 1\n",
        "    plt.subplot(3,3,pn)\n",
        "    plt.title(f'{score}({pd}/{gt})')\n",
        "    plt.imshow(test_x[i])"
      ],
      "metadata": {
        "id": "Yle2jaXEyUqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "수고하셨습니다!"
      ],
      "metadata": {
        "id": "kCrntC0SyDWQ"
      }
    }
  ]
}
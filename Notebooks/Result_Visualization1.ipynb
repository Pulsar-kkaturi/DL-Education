{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pulsar-kkaturi/DL-Education/blob/master/notebooks/Result_Visulization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37DOVvDMm-_A"
      },
      "source": [
        "# Result Visulization (결과 시각화)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4XY8H1nnFe_"
      },
      "source": [
        "# 1. Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vEPrq0skcWj"
      },
      "outputs": [],
      "source": [
        "import os, matplotlib, csv, shutil, json\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import skimage\n",
        "from skimage import transform as skit\n",
        "\n",
        "### Tensorflow 2.0 ###\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78f6Q_C-8dnh"
      },
      "source": [
        "# 2. 데이터셋 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwNtiL33oGpZ"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test)=keras.datasets.mnist.load_data(path='minist.npz')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP8qLluRuV2O"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hkS7G1ruu5q"
      },
      "source": [
        "# 2.1 MNIST Image Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UHKqWuXuWuu"
      },
      "outputs": [],
      "source": [
        "x_train_list = []\n",
        "x_test_list = []\n",
        "for i, i_ in enumerate(x_train[:1000]):\n",
        "    arr = skit.resize(x_train[i], (64, 64), anti_aliasing=True)\n",
        "    x_train_list.append(arr)\n",
        "for i, i_ in enumerate(x_test[:500]):\n",
        "    arr = skit.resize(x_test[i], (64, 64), anti_aliasing=True)\n",
        "    x_test_list.append(arr)\n",
        "\n",
        "x_train1 = np.expand_dims(np.array(x_train_list), axis=-1)\n",
        "x_test1 = np.expand_dims(np.array(x_test_list), axis=-1)\n",
        "print(x_train1.shape, x_test1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWvQRG19uzZF"
      },
      "source": [
        "## 2.2 MNIST LABEL Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrMj2eGFusoa"
      },
      "outputs": [],
      "source": [
        "y_train_list = []\n",
        "y_test_list = []\n",
        "for i, i_ in enumerate(y_train[:1000]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_train_list.append(zero)\n",
        "\n",
        "for i, i_ in enumerate(y_test[:500]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_test_list.append(zero)    \n",
        "    \n",
        "y_train1 = np.array(y_train_list)\n",
        "y_test1 = np.array(y_test_list)\n",
        "print(y_train1.shape, y_test1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2-6RHzfvi0N"
      },
      "source": [
        "## 2.3 MNIST figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM8fOyzMutQi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "num = 3\n",
        "for i in range(num):\n",
        "    for j in range(num):\n",
        "      id = (num*i) + j\n",
        "      plt.subplot(num,num,id+1)\n",
        "      plt.imshow(x_train1[id][...,0], cmap='gray')\n",
        "      plt.title('Class = {}'.format(y_train[id]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PsuhnRmvosv"
      },
      "source": [
        "# 3. VGG Model Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQyNwsW7vhw0"
      },
      "outputs": [],
      "source": [
        "def conv_block_2d(lr_conv, lr_num, par_list, bkn):\n",
        "        # parameter\n",
        "        filter_num = par_list[0]\n",
        "        conv_size = par_list[1]\n",
        "        conv_act = par_list[2]\n",
        "        pool_size = par_list[3]\n",
        "        # code\n",
        "        for i in range(lr_num):\n",
        "            lr_conv = layers.Conv2D(filter_num, conv_size, activation=None, padding='same', \n",
        "                                    kernel_initializer='he_normal',\n",
        "                                    name='block{}_conv{}'.format(bkn, i+1))(lr_conv)\n",
        "            lr_conv = layers.BatchNormalization(axis=-1, name='block{}_batchnorm{}'.format(bkn, i+1))(lr_conv)\n",
        "            lr_conv = layers.Activation(conv_act, name='block{}_activ{}'.format(bkn, i+1))(lr_conv)\n",
        "        lr_pool = layers.MaxPooling2D(pool_size=pool_size, name='block{}_pool'.format(bkn, i+1))(lr_conv)\n",
        "        return lr_pool\n",
        "\n",
        "def output_block(lr_dense, block_num, dens_count, act_func, drop_rate):\n",
        "    lr_dense = layers.Flatten(name='flatten_layer')(lr_dense)\n",
        "    for i in range(block_num):\n",
        "        lr_dense = layers.Dense(dens_count[i], kernel_regularizer=None,\n",
        "                                activation=act_func, name='classifier_dense_{}'.format(i+1))(lr_dense)\n",
        "        lr_dense = layers.Dropout(drop_rate, name='classifier_dropout_{}'.format(i+1))(lr_dense)\n",
        "    return lr_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ6tQ28ZwFmO"
      },
      "outputs": [],
      "source": [
        "def VGG16_2D(par_dic):\n",
        "    # parameters\n",
        "    input_size = par_dic['input_size']\n",
        "    conv_size = par_dic['conv_size']\n",
        "    conv_act = par_dic['conv_act']\n",
        "    pool_size = par_dic['pool_size']\n",
        "    dens_num = par_dic['dens_num']\n",
        "    dens_count = par_dic['dens_count']\n",
        "    dens_act = par_dic['dens_act']\n",
        "    drop_out = par_dic['drop_out']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "\n",
        "    # code block\n",
        "    inputs = Input(shape=(input_size, input_size, 1), name='input_layer')\n",
        "    block1 = conv_block_2d(inputs, 2, [64, conv_size, conv_act, pool_size])\n",
        "    block2 = conv_block_2d(block1, 2, [128, conv_size, conv_act, pool_size])\n",
        "    block3 = conv_block_2d(block2, 3, [256, conv_size, conv_act, pool_size])\n",
        "    block4 = conv_block_2d(block3, 3, [512, conv_size, conv_act, pool_size])\n",
        "    block5 = conv_block_2d(block4, 3, [512, conv_size, conv_act, pool_size])\n",
        "    dens = output_block(block5, dens_num, dens_count, dens_act, drop_out)\n",
        "    outputs = layers.Dense(output_count, activation=output_act, name='output_layer')(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEKz3nqgxYQQ"
      },
      "outputs": [],
      "source": [
        "def VGG19_2D(par_dic):\n",
        "    # parameters\n",
        "    input_size = par_dic['input_size']\n",
        "    conv_size = par_dic['conv_size']\n",
        "    conv_act = par_dic['conv_act']\n",
        "    pool_size = par_dic['pool_size']\n",
        "    dens_num = par_dic['dens_num']\n",
        "    dens_count = par_dic['dens_count']\n",
        "    dens_act = par_dic['dens_act']\n",
        "    drop_out = par_dic['drop_out']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "\n",
        "    # code block\n",
        "    inputs = Input(shape=(input_size, input_size, 1))\n",
        "    block1 = conv_block_2d(inputs, 2, [64, conv_size, conv_act, pool_size], 1)\n",
        "    block2 = conv_block_2d(block1, 2, [128, conv_size, conv_act, pool_size], 2)\n",
        "    block3 = conv_block_2d(block2, 4, [256, conv_size, conv_act, pool_size], 3)\n",
        "    block4 = conv_block_2d(block3, 4, [512, conv_size, conv_act, pool_size], 4)\n",
        "    block5 = conv_block_2d(block4, 4, [512, conv_size, conv_act, pool_size], 5)\n",
        "    dens = output_block(block5, dens_num, dens_count, dens_act, drop_out)\n",
        "    outputs = layers.Dense(output_count, activation=output_act)(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94txtM58xuKS"
      },
      "outputs": [],
      "source": [
        "network_param_set = {'input_size': 64,\n",
        "                     'conv_size': 3,\n",
        "                     'conv_act': 'relu',\n",
        "                     'pool_size': 2,\n",
        "                     'dens_num': 2,\n",
        "                     'dens_count': [1000,500],\n",
        "                     'dens_act': 'relu',\n",
        "                     'drop_out': 0.5,\n",
        "                     'output_count': 10,\n",
        "                     'output_act': 'softmax'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87IRklsi0_t0"
      },
      "outputs": [],
      "source": [
        "model = VGG19_2D(network_param_set)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRre4jFLxoXI"
      },
      "source": [
        "# 4. MNIST Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jhKYdmd119b"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=optimizers.Adam(lr=1e-4), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3tXYPUV17YC"
      },
      "outputs": [],
      "source": [
        "callback_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
        "                         keras.callbacks.ModelCheckpoint(filepath=os.path.join('model.h5'),\n",
        "                                                         monitor='val_loss', save_best_only=True),\n",
        "                         keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)]\n",
        "\n",
        "history = model.fit(x_train1, y_train1, epochs=50, batch_size=20, \n",
        "                    validation_data=(x_test1, y_test1),\n",
        "                    callbacks=callback_list, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVbOLIN1kFq4"
      },
      "source": [
        "# 5. Train Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPK_eXBMkLou"
      },
      "source": [
        "## 5.1. Loss & Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPAPGT9XkC0a"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,len(acc)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8uaVegLksW4"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc, 'b', color='blue', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', color='red', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', color='blue', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD0WC69Fk7af"
      },
      "source": [
        "##5.2. Prediction Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQL2Tcuak_86"
      },
      "outputs": [],
      "source": [
        "test1 = x_test1[0]\n",
        "print(test1.shape)\n",
        "plt.imshow(test1[...,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOqgXtWqlX11"
      },
      "outputs": [],
      "source": [
        "testp = x_test1[:100]\n",
        "testg = y_test[:100]\n",
        "scores = model.predict(testp)\n",
        "\n",
        "new_scores = []\n",
        "for score in scores:\n",
        "  max_val = np.max(score)\n",
        "  prob_num = list(score).index(max_val)\n",
        "  new_scores.append(prob_num)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ornqMTyJUov"
      },
      "outputs": [],
      "source": [
        "plt.imshow(testp[0,...,0])\n",
        "print(f'label={testg[0]}, predict={new_scores[0]}')\n",
        "print(scores[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrDS_3ZLgy7"
      },
      "source": [
        "### 5.2.1. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51cwU6mvLf3V"
      },
      "outputs": [],
      "source": [
        "conf = confusion_matrix(testg, new_scores, labels=[0,1,2,3,4,5,6,7,8,9])\n",
        "print(conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPTp7NnfMd4w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.heatmap(conf, annot = True, cmap=\"coolwarm\", vmax = 20, \n",
        "                 annot_kws={\"fontsize\":20}, center=10, cbar=True, \n",
        "                 xticklabels=list(range(10)), yticklabels=list(range(10)))\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels',fontsize=20);ax.set_ylabel('True labels',fontsize=20); \n",
        "ax.set_title('Confusion Matrix', fontsize=30); \n",
        "ax.xaxis.set_ticklabels(list(range(10)), fontsize=20); ax.yaxis.set_ticklabels(list(range(10)), fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsqVifQSmj7y"
      },
      "source": [
        "## 5.3. Class Attention Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKaUH9KbwxXv"
      },
      "source": [
        "먼저 GRAD-CAM을 구해주는 함수를 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwn_3iaRoJJz"
      },
      "outputs": [],
      "source": [
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n",
        "\n",
        "    # 1. 먼저 모델에서 마지막 컨브넷층을 output으로 하고, 테스트 이미지를 input으로 하는 모델을 구성한다.\n",
        "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # 2. 마지막 컨브넷층과 최종 예측결과를 인아웃으로 받는 모델을 구성한다.\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    for layer_name in classifier_layer_names:\n",
        "        x = model.get_layer(layer_name)(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # 그다음, 테스트 이미지에 대한 가장 높은 예측 클래스(여기서는 7)와 마지막 컨브넷층에 대한 그래디언트를 구한다.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 최종 컨브넷층의 결과 추출\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # 예측 클래스 계산\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # 마지막 컨브넷층의 특성맵에 대한 가장 높은 예측 클래스의 그래디언트.\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # 특성 맵 채널별 그래디언트 평균 값이 담긴 (512,) 크기의 벡터.\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # 가장 높은 예측 클래스에 대한 \"채널의 중요도\"를 특성 맵 배열의 채널에 곱한다.\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # 만들어진 특성 맵에서 채널 축을 따라 평균한 값이 클래스 활성화의 히트맵.\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # 마지막으로 구해진 히트맵을 0 ~ 1 사이값으로 정규화\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUkU8LUMy35r"
      },
      "outputs": [],
      "source": [
        "# MNIST 테스트 이미지에 대한 히트맵을 구한다.\n",
        "test1 = np.expand_dims(testp[0], axis=0)\n",
        "heatmap = make_gradcam_heatmap(test1, model, 'block5_activ4',['block5_pool','flatten_layer'])\n",
        "print(test1.shape, heatmap.shape)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC6S92Sk1Sf2"
      },
      "outputs": [],
      "source": [
        "# 히트맵을 0 ~255 사이 값으로 재조정\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# color map으로 변환\n",
        "cmap_sel = 'inferno'\n",
        "new_cm = plt.get_cmap(cmap_sel)\n",
        "\n",
        "# 컬러맵을 RGB 값으로 변환\n",
        "cm_colors = new_cm(np.arange(256))[:, :3]\n",
        "col_heatmap = cm_colors[heatmap]\n",
        "\n",
        "# RGB 컬러로 변환 컬러맵으로 원본크기에 맞게 히트맵 변환\n",
        "col_heatmap = keras.preprocessing.image.array_to_img(col_heatmap)\n",
        "col_heatmap = col_heatmap.resize((64, 64))\n",
        "col_heatmap = keras.preprocessing.image.img_to_array(col_heatmap)\n",
        "\n",
        "# 테스트 이미지에 히트맵 오버레이\n",
        "test1_col = test1 * [0.2989, 0.5870, 0.1140]\n",
        "superimposed_img = (col_heatmap * 0.005) + test1_col\n",
        "\n",
        "# 결과 시각화\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(test1_col[0], cmap='gray')\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(col_heatmap, cmap=cmap_sel)\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(superimposed_img[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1EjJnAZSqRr"
      },
      "source": [
        "### 5.3.1. CAM 모듈화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohFectDW1iLT"
      },
      "outputs": [],
      "source": [
        "def make_gradcam(img_arr):\n",
        "  img1 = np.expand_dims(img_arr, axis=0)\n",
        "  heatmap = make_gradcam_heatmap(img1, model, 'block5_activ4',['block5_pool','flatten_layer'])\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "  # color map으로 변환\n",
        "  cmap_sel = 'inferno'\n",
        "  new_cm = plt.get_cmap(cmap_sel)\n",
        "\n",
        "  # 컬러맵을 RGB 값으로 변환\n",
        "  cm_colors = new_cm(np.arange(256))[:, :3]\n",
        "  col_heatmap = cm_colors[heatmap]\n",
        "\n",
        "  # RGB 컬러로 변환 컬러맵으로 원본크기에 맞게 히트맵 변환\n",
        "  col_heatmap = keras.preprocessing.image.array_to_img(col_heatmap)\n",
        "  col_heatmap = col_heatmap.resize((64, 64))\n",
        "  col_heatmap = keras.preprocessing.image.img_to_array(col_heatmap)\n",
        "\n",
        "  # 테스트 이미지에 히트맵 오버레이\n",
        "  test1_col = img1 * [0.2989, 0.5870, 0.1140]\n",
        "  superimposed_img = (col_heatmap * 0.005) + test1_col\n",
        "  return test1_col[0], superimposed_img[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJJHHRkVXCU"
      },
      "outputs": [],
      "source": [
        "def number_extractor(labels, num=0):\n",
        "  id_list = []\n",
        "  n = 0\n",
        "  for id, l in enumerate(labels):\n",
        "    if int(l) == num and n < 5:\n",
        "      id_list.append(id)\n",
        "      n+=1\n",
        "  return id_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyLS8tB7SpvS"
      },
      "outputs": [],
      "source": [
        "def grad_cam_show(idx_list):\n",
        "  plt.figure(figsize=(20,10))\n",
        "  for i, id in enumerate(idx_list):\n",
        "    img, cam = make_gradcam(testp[id])\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f'label = {testg[id]}')\n",
        "    plt.subplot(2,5,6+i)\n",
        "    plt.imshow(cam)\n",
        "    plt.title(f'label = {new_scores[id]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4EI8nWxW98G"
      },
      "outputs": [],
      "source": [
        "grad_cam_show(number_extractor(testg, num=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NZDPBlIW2iQ"
      },
      "outputs": [],
      "source": [
        "grad_cam_show(number_extractor(testg, num=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdbdg9nDTXzP"
      },
      "outputs": [],
      "source": [
        "grad_cam_show(number_extractor(testg, num=9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhKrL7jIXsYi"
      },
      "source": [
        "## 5.4. 필터 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4kJ30ZydKT4"
      },
      "source": [
        "### 5.4.1. Feature Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZicbnZ2tXvFb"
      },
      "outputs": [],
      "source": [
        "layer_outputs = []\n",
        "layer_names = []\n",
        "for layer in model.layers:\n",
        "  if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
        "    layer_outputs.append(layer.output)\n",
        "    layer_names.append(layer.name)\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27u_j8AgZqsj"
      },
      "outputs": [],
      "source": [
        "test_img = np.expand_dims(testp[0], axis=0)\n",
        "print(test_img.shape)\n",
        "plt.imshow(test_img[0,...,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A03chMG5YrZ_"
      },
      "outputs": [],
      "source": [
        "act_result = activation_model.predict(test_img)\n",
        "print(len(act_result))\n",
        "for act in act_result:\n",
        "  print(act.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj5S-jvmY-Bq"
      },
      "outputs": [],
      "source": [
        "def show_feature_map(features, all_mode=True):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  if all_mode:\n",
        "    chn = features.shape[-1]\n",
        "    chns = np.sqrt(chn)\n",
        "  else:\n",
        "    chn = 16\n",
        "    chns = 4\n",
        "  for i in range(chn):\n",
        "    plt.subplot(chns, chns, i+1)\n",
        "    plt.imshow(features[0,...,i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5glCoZXta7Ee"
      },
      "outputs": [],
      "source": [
        "show_feature_map(act_result[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig5WIAsybT9J"
      },
      "outputs": [],
      "source": [
        "show_feature_map(act_result[15], all_mode=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Result_Visulization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a931a7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Pulsar-kkaturi/DL-Education/blob/master/VisionDL_Lecture/Lecture5_ResultAnalysis_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed204f",
   "metadata": {},
   "source": [
    "# Result Visualization (결과 시각화)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90273185",
   "metadata": {},
   "source": [
    "# 1. Library Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de652a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, matplotlib, csv, shutil, json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skimage\n",
    "from skimage import transform as skit\n",
    "from skimage import filters as skif\n",
    "\n",
    "### PyTorch ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee92c92",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ef822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 데이터셋 로딩 (PyTorch 방식)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# NumPy 배열로 변환\n",
    "x_train = (train_dataset.data.astype(np.float32) / 255.0)\n",
    "y_train = np.array(train_dataset.targets)\n",
    "x_test = (test_dataset.data.astype(np.float32) / 255.0)\n",
    "y_test = np.array(test_dataset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494f8ab",
   "metadata": {},
   "source": [
    "# 2.1 Cifar10 to Cifar2 (airplane, automobile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "n0, n1 = 0, 0\n",
    "t0, t1 = 0, 0\n",
    "\n",
    "for i, i_ in enumerate(x_train):\n",
    "    if y_train[i] == 0 and n0 < 500:\n",
    "        arr = skit.resize(i_, (64, 64), anti_aliasing=True)\n",
    "        onehot = [1, 0]\n",
    "        n0 += 1\n",
    "        x_train_list.append(arr)\n",
    "        y_train_list.append(onehot)\n",
    "    elif y_train[i] == 1 and n1 < 500:\n",
    "        arr = skit.resize(i_, (64, 64), anti_aliasing=True)\n",
    "        onehot = [0, 1]\n",
    "        n1 += 1\n",
    "        x_train_list.append(arr)\n",
    "        y_train_list.append(onehot)\n",
    "\n",
    "for i, i_ in enumerate(x_test):\n",
    "    if y_test[i] == 0 and t0 < 100:\n",
    "        arr = skit.resize(i_, (64, 64), anti_aliasing=True)\n",
    "        onehot = [1, 0]\n",
    "        t0 += 1\n",
    "        x_test_list.append(arr)\n",
    "        y_test_list.append(onehot)\n",
    "    elif y_test[i] == 1 and t1 < 100:\n",
    "        arr = skit.resize(i_, (64, 64), anti_aliasing=True)\n",
    "        onehot = [0, 1]\n",
    "        t1 += 1\n",
    "        x_test_list.append(arr)\n",
    "        y_test_list.append(onehot)\n",
    "\n",
    "train_x = np.array(x_train_list)\n",
    "train_y = np.array(y_train_list)\n",
    "print(train_x.shape, train_y.shape)\n",
    "test_x = np.array(x_test_list)\n",
    "test_y = np.array(y_test_list)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "# PyTorch tensor로 변환 (channels first 형태로)\n",
    "train_x_tensor = torch.FloatTensor(train_x).permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "train_y_tensor = torch.FloatTensor(train_y)\n",
    "test_x_tensor = torch.FloatTensor(test_x).permute(0, 3, 1, 2)\n",
    "test_y_tensor = torch.FloatTensor(test_y)\n",
    "\n",
    "print(f\"PyTorch tensor shapes:\")\n",
    "print(f\"train_x_tensor: {train_x_tensor.shape}, train_y_tensor: {train_y_tensor.shape}\")\n",
    "print(f\"test_x_tensor: {test_x_tensor.shape}, test_y_tensor: {test_y_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810aad6c",
   "metadata": {},
   "source": [
    "## 2.2 Cifar2 figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83149138",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['ariplane', 'automobile']\n",
    "plt.figure(figsize=(10,10))\n",
    "num = 3\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        id = (num*i) + j\n",
    "        plt.subplot(num,num,id+1)\n",
    "        plt.imshow(train_x[id])\n",
    "        plt.title('Class = {}'.format(label_list[list(train_y[id]).index(1)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb957187",
   "metadata": {},
   "source": [
    "# 3. VGG Model Build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_size, num_layers, pool_size, conv_act='relu'):\n",
    "        super(ConvBlock2D, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, conv_size, padding=1))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(out_channels, out_channels, conv_size, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if conv_act == 'relu':\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.MaxPool2d(kernel_size=pool_size, stride=pool_size))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_features, dens_count, dens_act='relu', drop_rate=0.5, output_count=2):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        prev_features = in_features\n",
    "        for i, features in enumerate(dens_count):\n",
    "            layers.append(nn.Linear(prev_features, features))\n",
    "            if dens_act == 'relu':\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            prev_features = features\n",
    "        \n",
    "        layers.append(nn.Linear(prev_features, output_count))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_2D(nn.Module):\n",
    "    def __init__(self, par_dic):\n",
    "        super(VGG16_2D, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_size = par_dic['input_size']\n",
    "        self.channels = par_dic['channels']\n",
    "        conv_size = par_dic['conv_size']\n",
    "        conv_act = par_dic['conv_act']\n",
    "        pool_size = par_dic['pool_size']\n",
    "        dens_num = par_dic['dens_num']\n",
    "        dens_count = par_dic['dens_count']\n",
    "        dens_act = par_dic['dens_act']\n",
    "        drop_out = par_dic['drop_out']\n",
    "        output_count = par_dic['output_count']\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.block1 = ConvBlock2D(self.channels, 64, conv_size, 2, pool_size, conv_act)\n",
    "        self.block2 = ConvBlock2D(64, 128, conv_size, 2, pool_size, conv_act)\n",
    "        self.block3 = ConvBlock2D(128, 256, conv_size, 3, pool_size, conv_act)\n",
    "        self.block4 = ConvBlock2D(256, 512, conv_size, 3, pool_size, conv_act)\n",
    "        self.block5 = ConvBlock2D(512, 512, conv_size, 3, pool_size, conv_act)\n",
    "        \n",
    "        # Calculate the size after convolutions for the classifier\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, self.channels, self.input_size, self.input_size)\n",
    "            dummy_features = self._forward_features(dummy_input)\n",
    "            flattened_size = dummy_features.view(1, -1).size(1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = OutputBlock(flattened_size, dens_count, dens_act, drop_out, output_count)\n",
    "        \n",
    "        # Store feature names for Grad-CAM\n",
    "        self.feature_layers = {\n",
    "            'block1': self.block1,\n",
    "            'block2': self.block2,\n",
    "            'block3': self.block3,\n",
    "            'block4': self.block4,\n",
    "            'block5': self.block5\n",
    "        }\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self._forward_features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19_2D(nn.Module):\n",
    "    def __init__(self, par_dic):\n",
    "        super(VGG19_2D, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_size = par_dic['input_size']\n",
    "        self.channels = par_dic['channels']\n",
    "        conv_size = par_dic['conv_size']\n",
    "        conv_act = par_dic['conv_act']\n",
    "        pool_size = par_dic['pool_size']\n",
    "        dens_num = par_dic['dens_num']\n",
    "        dens_count = par_dic['dens_count']\n",
    "        dens_act = par_dic['dens_act']\n",
    "        drop_out = par_dic['drop_out']\n",
    "        output_count = par_dic['output_count']\n",
    "        \n",
    "        # Feature extraction layers (VGG19 has more layers)\n",
    "        self.block1 = ConvBlock2D(self.channels, 64, conv_size, 2, pool_size, conv_act)\n",
    "        self.block2 = ConvBlock2D(64, 128, conv_size, 2, pool_size, conv_act)\n",
    "        self.block3 = ConvBlock2D(128, 256, conv_size, 4, pool_size, conv_act)\n",
    "        self.block4 = ConvBlock2D(256, 512, conv_size, 4, pool_size, conv_act)\n",
    "        self.block5 = ConvBlock2D(512, 512, conv_size, 4, pool_size, conv_act)\n",
    "        \n",
    "        # Calculate the size after convolutions for the classifier\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, self.channels, self.input_size, self.input_size)\n",
    "            dummy_features = self._forward_features(dummy_input)\n",
    "            flattened_size = dummy_features.view(1, -1).size(1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = OutputBlock(flattened_size, dens_count, dens_act, drop_out, output_count)\n",
    "        \n",
    "        # Store feature names for Grad-CAM\n",
    "        self.feature_layers = {\n",
    "            'block1': self.block1,\n",
    "            'block2': self.block2,\n",
    "            'block3': self.block3,\n",
    "            'block4': self.block4,\n",
    "            'block5': self.block5\n",
    "        }\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self._forward_features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_param_set = {'input_size': 64,\n",
    "                     'channels': 3,\n",
    "                     'conv_size': 3,\n",
    "                     'conv_act': 'relu',\n",
    "                     'pool_size': 2,\n",
    "                     'dens_num': 2,\n",
    "                     'dens_count': [1000,500],\n",
    "                     'dens_act': 'relu',\n",
    "                     'drop_out': 0.5,\n",
    "                     'output_count': 2,\n",
    "                     'output_act': 'softmax'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_2D(network_param_set).to(device)\n",
    "print(model)\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5405a48",
   "metadata": {},
   "source": [
    "# 4. CIFAR2 Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function과 optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Convert one-hot to class indices\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target_indices.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Convert one-hot to class indices\n",
    "            target_indices = torch.argmax(target, dim=1)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target_indices)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target_indices.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(test_loader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# History 저장용\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 학습\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # 검증\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # History 저장\n",
    "    history['loss'].append(train_loss)\n",
    "    history['accuracy'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 최고 모델 저장\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "# 최고 모델 로드\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f698c",
   "metadata": {},
   "source": [
    "# 5. Train Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141c35a",
   "metadata": {},
   "source": [
    "## 5.1. Loss & Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc1cc4",
   "metadata": {},
   "source": [
    "##5.2. Prediction Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2386a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test_x[0]\n",
    "print(test1.shape)\n",
    "plt.imshow(test1)\n",
    "plt.title(label_list[list(test_y[0]).index(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba61a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_x_tensor_device = test_x_tensor.to(device)\n",
    "    scores = model(test_x_tensor_device)\n",
    "    scores = F.softmax(scores, dim=1)  # softmax 적용\n",
    "    scores = scores.cpu().numpy()  # CPU로 이동 후 numpy 변환\n",
    "\n",
    "new_scores = []\n",
    "for score in scores:\n",
    "    max_val = np.max(score)\n",
    "    prob_num = label_list[list(score).tolist().index(max_val)]\n",
    "    new_scores.append(prob_num)\n",
    "print(new_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "for y in test_y:\n",
    "    max_val = np.max(y)\n",
    "    prob_num = label_list[list(y).index(max_val)]\n",
    "    new_labels.append(prob_num)\n",
    "print(new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_x[0])\n",
    "print(f'label={label_list[list(test_y[0]).index(1)]}, predict={new_scores[0]}')\n",
    "print(scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df763e",
   "metadata": {},
   "source": [
    "### 5.3. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb90f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(new_labels, new_scores, labels=label_list)\n",
    "print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(conf, annot = True, cmap=\"coolwarm\", vmax = 100,\n",
    "                 annot_kws={\"fontsize\":20}, center=50, cbar=True,\n",
    "                 xticklabels=label_list, yticklabels=label_list)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels',fontsize=20);ax.set_ylabel('True labels',fontsize=20);\n",
    "ax.set_title('Confusion Matrix', fontsize=30);\n",
    "ax.xaxis.set_ticklabels(label_list, fontsize=20); ax.yaxis.set_ticklabels(label_list, fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725f052",
   "metadata": {},
   "source": [
    "# 6. Result Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e01dc2",
   "metadata": {},
   "source": [
    "## 6.1. Class Activation Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01158e76",
   "metadata": {},
   "source": [
    "먼저 GRAD-CAM을 구해주는 함수를 정의한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Hook 등록\n",
    "        self.target_layer.register_forward_hook(self.save_feature_maps)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "    \n",
    "    def save_feature_maps(self, module, input, output):\n",
    "        self.feature_maps = output\n",
    "    \n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def generate_cam(self, input_image, class_idx=None):\n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output, dim=1)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output[:, class_idx].backward(retain_graph=True)\n",
    "        \n",
    "        # Generate CAM\n",
    "        gradients = self.gradients[0]  # (C, H, W)\n",
    "        feature_maps = self.feature_maps[0]  # (C, H, W)\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # (C,)\n",
    "        \n",
    "        # Weighted combination of feature maps\n",
    "        cam = torch.zeros(feature_maps.shape[1:], dtype=torch.float32, device=feature_maps.device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * feature_maps[i, :, :]\n",
    "        \n",
    "        # Apply ReLU\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "        \n",
    "        return cam.cpu().numpy()\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, target_layer_name):\n",
    "    # target_layer 찾기\n",
    "    target_layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if target_layer_name in name:\n",
    "            target_layer = module\n",
    "            break\n",
    "    \n",
    "    if target_layer is None:\n",
    "        print(f\"Target layer {target_layer_name} not found!\")\n",
    "        return None\n",
    "    \n",
    "    # GradCAM 객체 생성\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # CAM 생성\n",
    "    heatmap = grad_cam.generate_cam(img_array)\n",
    "    \n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지에 대한 히트맵을 구한다.\n",
    "test1_tensor = test_x_tensor[0:1].to(device)  # 배치 차원 추가\n",
    "heatmap1 = make_gradcam_heatmap(test1_tensor, model, 'block4')\n",
    "heatmap2 = make_gradcam_heatmap(test1_tensor, model, 'block5')\n",
    "print(test1_tensor.shape, heatmap1.shape, heatmap2.shape)\n",
    "\n",
    "# Display heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(heatmap1)\n",
    "plt.title('Block4 Heatmap')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(heatmap2)\n",
    "plt.title('Block5 Heatmap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본크기에 맞게 히트맵 변환\n",
    "hm1 = skit.resize(heatmap1, (64, 64), anti_aliasing=True)\n",
    "hm1 = skif.gaussian(hm1, sigma=3)\n",
    "hm2 = skit.resize(heatmap2, (64, 64), anti_aliasing=True)\n",
    "hm2 = skif.gaussian(hm2, sigma=3)\n",
    "hmn = (hm1+hm2)/2\n",
    "\n",
    "# 히트맵을 0 ~255 사이 값으로 재조정\n",
    "heatmap_n = np.uint8(255 * hmn)\n",
    "test_n = np.uint8(255*test_x[0])\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_n)\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(heatmap_n)\n",
    "plt.title('Heatmap')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_n)\n",
    "plt.imshow(heatmap_n, alpha=0.5)\n",
    "plt.title('Overlay')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a74c4b",
   "metadata": {},
   "source": [
    "### 6.2. CAM 모듈화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam(img_arr):\n",
    "    # NumPy 이미지를 PyTorch tensor로 변환\n",
    "    if isinstance(img_arr, np.ndarray):\n",
    "        if len(img_arr.shape) == 3:  # (H, W, C)\n",
    "            img_tensor = torch.FloatTensor(img_arr).permute(2, 0, 1).unsqueeze(0)  # (1, C, H, W)\n",
    "        else:  # 이미 (C, H, W) 형태\n",
    "            img_tensor = torch.FloatTensor(img_arr).unsqueeze(0)\n",
    "    else:\n",
    "        img_tensor = img_arr\n",
    "    \n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    hm1 = make_gradcam_heatmap(img_tensor, model, 'block4')\n",
    "    hm2 = make_gradcam_heatmap(img_tensor, model, 'block5')\n",
    "\n",
    "    # 원본크기에 맞게 히트맵 변환\n",
    "    hm1 = skit.resize(hm1, (64, 64), anti_aliasing=True)\n",
    "    hm1 = skif.gaussian(hm1, sigma=3)\n",
    "    hm2 = skit.resize(hm2, (64, 64), anti_aliasing=True)\n",
    "    hm2 = skif.gaussian(hm2, sigma=3)\n",
    "    hmn = (hm1+hm2)/2\n",
    "\n",
    "    # 히트맵을 0 ~255 사이 값으로 재조정\n",
    "    heatmap1 = np.uint8(255 * hmn)\n",
    "    \n",
    "    # 원본 이미지도 0-255 범위로\n",
    "    if isinstance(img_arr, np.ndarray):\n",
    "        img1 = np.uint8(255 * img_arr)\n",
    "    else:\n",
    "        img1 = np.uint8(255 * img_arr)\n",
    "    \n",
    "    return img1, heatmap1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ad2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "n, s = 5, 0\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        idx = (i*n)+j+s\n",
    "        if idx >= len(test_x):\n",
    "            break\n",
    "        img, hm = make_gradcam(test_x[idx])\n",
    "        plt.subplot(n,n,(i*n)+j+1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(hm, alpha=0.5, cmap='Reds')\n",
    "        if new_scores[idx] == new_labels[idx]:\n",
    "            cor = 'True'\n",
    "        else:\n",
    "            cor = 'False'\n",
    "        plt.title(f'({cor})')\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e5605",
   "metadata": {},
   "source": [
    "## 6.3. Feature Map 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook을 사용하여 중간 층의 출력을 저장하는 클래스\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model, layers):\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.features = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        for layer_name in layers:\n",
    "            layer = dict(model.named_modules())[layer_name]\n",
    "            hook = layer.register_forward_hook(self.get_features(layer_name))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def get_features(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.features[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "# 특정 층들의 feature map을 추출\n",
    "feature_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.MaxPool2d)):\n",
    "        feature_layers.append(name)\n",
    "\n",
    "print(\"Available feature layers:\")\n",
    "for layer in feature_layers:\n",
    "    print(f\"  {layer}\")\n",
    "\n",
    "# Feature extractor 생성\n",
    "extractor = FeatureExtractor(model, feature_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f48de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지를 모델에 통과시켜 feature map 추출\n",
    "test_img_tensor = test_x_tensor[0:1].to(device)\n",
    "print(test_img_tensor.shape)\n",
    "\n",
    "# 이미지 표시 (원본)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(test_x[0])\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Forward pass를 통해 feature map들 추출\n",
    "with torch.no_grad():\n",
    "    _ = model(test_img_tensor)\n",
    "\n",
    "# 추출된 feature map들 확인\n",
    "print(\"\\nExtracted feature maps:\")\n",
    "for layer_name, features in extractor.features.items():\n",
    "    print(f\"{layer_name}: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fab8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_map(features, all_mode=True):\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = features.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    if all_mode:\n",
    "        chn = features.shape[1]  # PyTorch는 (N, C, H, W)\n",
    "        chns = int(np.ceil(np.sqrt(chn)))\n",
    "    else:\n",
    "        chn = min(16, features.shape[1])\n",
    "        chns = 4\n",
    "    \n",
    "    for i in range(chn):\n",
    "        plt.subplot(chns, chns, i+1)\n",
    "        plt.imshow(features[0, i, ...], cmap='viridis')  # (N, C, H, W) -> (H, W)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f373a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 conv layer의 feature map 시각화\n",
    "first_conv_features = None\n",
    "for layer_name, features in extractor.features.items():\n",
    "    if 'conv' in layer_name.lower():\n",
    "        first_conv_features = features\n",
    "        print(f\"Showing feature maps from: {layer_name}\")\n",
    "        break\n",
    "\n",
    "if first_conv_features is not None:\n",
    "    show_feature_map(first_conv_features, all_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 층의 feature map을 한번에 시각화\n",
    "plt.figure(figsize=(20, 30))\n",
    "layer_count = 0\n",
    "\n",
    "for layer_name, features in extractor.features.items():\n",
    "    if layer_count >= 10:  # 최대 10개 층만 표시\n",
    "        break\n",
    "    \n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features_np = features.cpu().numpy()\n",
    "    else:\n",
    "        features_np = features\n",
    "    \n",
    "    # 각 층에서 첫 4개 채널만 표시\n",
    "    num_channels = min(4, features_np.shape[1])\n",
    "    \n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(10, 4, (layer_count * 4) + j + 1)\n",
    "        plt.imshow(features_np[0, j, ...], cmap='viridis')\n",
    "        plt.title(f'{layer_name}\\nCh {j}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    layer_count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hook 제거\n",
    "extractor.remove_hooks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef8b22",
   "metadata": {},
   "source": [
    "## 6.4. Filter 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 시각화를 위한 클래스\n",
    "class FilterVisualizer:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.target_layer = None\n",
    "        \n",
    "        # 타겟 레이어 찾기\n",
    "        for name, module in model.named_modules():\n",
    "            if name == layer_name:\n",
    "                self.target_layer = module\n",
    "                break\n",
    "        \n",
    "        if self.target_layer is None:\n",
    "            raise ValueError(f\"Layer {layer_name} not found in model\")\n",
    "    \n",
    "    def compute_loss(self, image, filter_index):\n",
    "        # Feature extractor 생성\n",
    "        feature_extractor = FeatureExtractor(self.model, [self.layer_name])\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = self.model(image)\n",
    "        \n",
    "        # 해당 레이어의 출력 가져오기\n",
    "        activation = feature_extractor.features[self.layer_name]\n",
    "        \n",
    "        # 특정 필터의 활성화 평균\n",
    "        if len(activation.shape) == 4:  # (N, C, H, W)\n",
    "            filter_activation = activation[:, filter_index, 2:-2, 2:-2]\n",
    "        else:\n",
    "            filter_activation = activation[:, filter_index]\n",
    "        \n",
    "        loss = torch.mean(filter_activation)\n",
    "        \n",
    "        # Hook 제거\n",
    "        feature_extractor.remove_hooks()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def gradient_ascent_step(self, image, filter_index, learning_rate):\n",
    "        image.requires_grad_(True)\n",
    "        \n",
    "        loss = self.compute_loss(image, filter_index)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient ascent\n",
    "        with torch.no_grad():\n",
    "            # L2 정규화\n",
    "            grads = image.grad\n",
    "            grads = grads / (torch.norm(grads) + 1e-8)\n",
    "            \n",
    "            # 업데이트\n",
    "            image += learning_rate * grads\n",
    "            \n",
    "            # 그래디언트 초기화\n",
    "            image.grad.zero_()\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def generate_filter_pattern(self, filter_index, img_width=64, img_height=64, iterations=30, learning_rate=10.0):\n",
    "        # 랜덤 이미지 생성\n",
    "        image = torch.rand(1, 3, img_height, img_width, device=device) * 0.2 + 0.4\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            image = self.gradient_ascent_step(image, filter_index, learning_rate)\n",
    "        \n",
    "        return image[0].detach().cpu().numpy()\n",
    "    \n",
    "    def deprocess_image(self, image, margin=0.1):\n",
    "        # 채널을 마지막으로 이동: (C, H, W) -> (H, W, C)\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        \n",
    "        # 정규화\n",
    "        image -= np.mean(image)\n",
    "        image /= (np.std(image) + 1e-8)\n",
    "        image *= 64\n",
    "        image += 128\n",
    "        image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # 마진 제거\n",
    "        margin_pixels = int(image.shape[0] * margin)\n",
    "        if margin_pixels > 0:\n",
    "            image = image[margin_pixels:-margin_pixels, margin_pixels:-margin_pixels, :]\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def visualize_filters(self, num_filters=25, width=64, height=64):\n",
    "        filters_per_row = int(np.sqrt(num_filters))\n",
    "        \n",
    "        all_images = []\n",
    "        for filter_index in tqdm(range(num_filters), desc=\"Generating filter visualizations\"):\n",
    "            try:\n",
    "                raw_image = self.generate_filter_pattern(filter_index, width, height)\n",
    "                processed_image = self.deprocess_image(raw_image, 0.1)\n",
    "                all_images.append(processed_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating filter {filter_index}: {e}\")\n",
    "                # 빈 이미지 추가\n",
    "                all_images.append(np.zeros((height-int(height*0.2), width-int(width*0.2), 3), dtype=np.uint8))\n",
    "        \n",
    "        # 시각화\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(filters_per_row):\n",
    "            for j in range(filters_per_row):\n",
    "                idx = i * filters_per_row + j\n",
    "                if idx < len(all_images):\n",
    "                    plt.subplot(filters_per_row, filters_per_row, idx + 1)\n",
    "                    plt.imshow(all_images[idx])\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f'Filter {idx}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def filter_visual(layer_name, width=64, height=64):\n",
    "    try:\n",
    "        visualizer = FilterVisualizer(model, layer_name)\n",
    "        visualizer.visualize_filters(25, width, height)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in filter visualization: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810ce1c",
   "metadata": {},
   "source": [
    "### 6.4.1. cifar2 result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54931e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 Conv2D 레이어들 확인\n",
    "conv_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        conv_layers.append(name)\n",
    "        print(name)\n",
    "\n",
    "print(f\"\\nTotal Conv2D layers found: {len(conv_layers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block5의 첫 번째 conv layer 필터 시각화\n",
    "if len(conv_layers) > 0:\n",
    "    # block5의 conv layer 찾기\n",
    "    block5_conv = None\n",
    "    for layer_name in conv_layers:\n",
    "        if 'block5' in layer_name and 'conv' in layer_name:\n",
    "            block5_conv = layer_name\n",
    "            break\n",
    "    \n",
    "    if block5_conv:\n",
    "        print(f\"Visualizing filters from: {block5_conv}\")\n",
    "        filter_visual(block5_conv, 64, 64)\n",
    "    else:\n",
    "        # 대안으로 마지막 conv layer 사용\n",
    "        print(f\"Block5 conv not found, using: {conv_layers[-1]}\")\n",
    "        filter_visual(conv_layers[-1], 64, 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be2027",
   "metadata": {},
   "source": [
    "### 6.4.2. ImageNet Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc01e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet pretrained VGG16 로드\n",
    "import torchvision.models as models\n",
    "\n",
    "vgg16_pretrained = models.vgg16(pretrained=True).to(device)\n",
    "vgg16_pretrained.eval()\n",
    "\n",
    "# Conv2D 레이어들 확인\n",
    "imagenet_conv_layers = []\n",
    "for name, module in vgg16_pretrained.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        imagenet_conv_layers.append(name)\n",
    "        print(name)\n",
    "\n",
    "print(f\"\\nTotal Conv2D layers in ImageNet VGG16: {len(imagenet_conv_layers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet VGG16의 특정 층 필터 시각화\n",
    "if len(imagenet_conv_layers) > 0:\n",
    "    # features.28은 대략 block5_conv1에 해당\n",
    "    target_layer = 'features.28'  # 또는 다른 적절한 레이어\n",
    "    if target_layer in imagenet_conv_layers:\n",
    "        print(f\"Visualizing ImageNet VGG16 filters from: {target_layer}\")\n",
    "        \n",
    "        # ImageNet용 filter visualizer\n",
    "        imagenet_visualizer = FilterVisualizer(vgg16_pretrained, target_layer)\n",
    "        imagenet_visualizer.visualize_filters(25, 256, 256)\n",
    "    else:\n",
    "        print(f\"Target layer not found, available layers: {imagenet_conv_layers[:5]}...\")  # 처음 5개만 표시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528434f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd46fac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pulsar-kkaturi/DL-Education/blob/master/Special_Lecture/Lecture4_CNNBuild.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDCIqpTnJeqZ"
      },
      "source": [
        "# Lecture 3. CNN Build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTaaZF3HJs40"
      },
      "source": [
        "## 1. Simple CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwbSpz2l8v7_"
      },
      "source": [
        "### 1.1. Library Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vIIvdH4_JbR2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 14:24:49.953864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-05 14:24:50.658534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os, matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "### Tensorflow 2.0 ###\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-5JgCVTgBH9s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSc3RZmiKDUv"
      },
      "source": [
        "### 1.2. 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H6Z_blzwKB16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28) (60000,)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data(path='minist.npz')\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FLSAWYcKIjl"
      },
      "outputs": [],
      "source": [
        "x_train_list = []\n",
        "x_test_list = []\n",
        "for i, i_ in enumerate(x_train[:1000]):\n",
        "    arr = np.zeros(shape=(32, 32))\n",
        "    arr[:28,:28] = x_train[i]\n",
        "    x_train_list.append(arr)\n",
        "for i, i_ in enumerate(x_test[:500]):\n",
        "    arr = np.zeros(shape=(32, 32))\n",
        "    arr[:28,:28] = x_test[i]\n",
        "    x_test_list.append(arr)\n",
        "\n",
        "x_train1 = np.expand_dims(np.array(x_train_list), axis=-1)\n",
        "x_test1 = np.expand_dims(np.array(x_test_list), axis=-1)\n",
        "print(x_train1.shape, x_test1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvzy72PlM64n"
      },
      "outputs": [],
      "source": [
        "y_train_list = []\n",
        "y_test_list = []\n",
        "for i, i_ in enumerate(y_train[:1000]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_train_list.append(zero)\n",
        "\n",
        "for i, i_ in enumerate(y_test[:500]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_test_list.append(zero)    \n",
        "    \n",
        "y_train1 = np.array(y_train_list)\n",
        "y_test1 = np.array(y_test_list)\n",
        "print(y_train1.shape, y_test1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_U2eCQPKNp0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(x_train1[i][...,0], cmap='gray')\n",
        "    plt.title('Class = {}'.format(y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhgW17uKtdU"
      },
      "source": [
        "### 1.3. 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6UJSQiSK1dr"
      },
      "source": [
        "# **AI 모델을 구성하는 레이어 만들기**\n",
        "\n",
        "AI 모델은 여러 개의 레이어를 쌓아 올려 만듭니다.  \n",
        "가장 대표적인 레이어 구조인 **CONV-BN-ACT-POOL** 구조를 만들어 보겠습니다.\n",
        "\n",
        "먼저 데이터가 들어가는 첫 번째 레이어를 만들어 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9IJsyASKTTq"
      },
      "outputs": [],
      "source": [
        "first_layer = Input(shape=(32, 32, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQIZ8FEDK51O"
      },
      "source": [
        "그 다음으로 데이터의 특징을 추출할 Convolution 레이어를 연결하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ft6sLtsK859"
      },
      "outputs": [],
      "source": [
        "second_layer = layers.Conv2D(filters=8, kernel_size=(3, 3), activation=None, padding='same')(first_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vizq1nu8LGlU"
      },
      "source": [
        "다음으로 레이어 중간에서 정규화를 도와줄 Batch Normalization 레이어를 추가하겠습니다.다음으로 레이어 중간에서 정규화를 도와줄 Batch Normalization 레이어를 추가하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUmnPVdhK-oo"
      },
      "outputs": [],
      "source": [
        "third_layer = layers.BatchNormalization()(second_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZprgalLPSn"
      },
      "source": [
        "Batch Normalization 이후 신호를 변환하여 다음 뉴런으로 전달하는 Activation function 레이어를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idrmW7DWLMUH"
      },
      "outputs": [],
      "source": [
        "fourth_layer = layers.Activation('relu')(third_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yP4a-DALVz7"
      },
      "source": [
        "다음으로 이미지 사이즈를 줄여주는 Pooling 레이어를 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce7ptoGMLRbp"
      },
      "outputs": [],
      "source": [
        "fifth_layer = layers.MaxPool2D(strides=(2, 2))(fourth_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOPLGDOmLbl3"
      },
      "source": [
        "그 후 모든 뉴런을 일렬로 늘어세우는 Flatten 레이어를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qshjYfELY00"
      },
      "outputs": [],
      "source": [
        "sixth_layer = layers.Flatten()(fifth_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4puhZMLi75"
      },
      "source": [
        "일렬로 늘어세운 후 이전 계층의 모든 뉴런을 연결해주는 Fully connected(Dense) 레이어를 연결합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Au2f0SaLgLw"
      },
      "outputs": [],
      "source": [
        "seventh_layer = layers.Dense(100, activation = 'relu')(sixth_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wslnvsGLtXH"
      },
      "source": [
        "Dropout 레이어를 활용해 일부 뉴런들을 무작위로 학습에서 배제하도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHlH9exdLnXE"
      },
      "outputs": [],
      "source": [
        "eighth_layer = layers.Dropout(0.25)(seventh_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKE18CBDLy3y"
      },
      "source": [
        "마지막으로 최종 결과물을 출력해주는 레이어를 만들어 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JBcDxmpLpzF"
      },
      "outputs": [],
      "source": [
        "final_layer =  layers.Dense(10, activation='sigmoid')(eighth_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzBKkHcLL8u-"
      },
      "source": [
        "지금까지 만든 레이어를 Model 함수에 넣어 연결하면 모델이 완성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke96nkx5L1_7"
      },
      "outputs": [],
      "source": [
        "model = Model(first_layer, final_layer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYbBVGh7MI2k"
      },
      "source": [
        "### 1.4. 모델 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RkQLvYpMlV7"
      },
      "source": [
        "신경망 모델의 손실함수와 옵티마이저, 학습률 등의 파라미터를 지정해줍니다. \n",
        "\n",
        "성능은 정확도를 평가할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9asGMjBL9Un"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=optimizers.Adam(lr=1e-4), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54u7L5V0MW4D"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train1, y_train1, epochs=20, batch_size=32, \n",
        "                    validation_data=(x_test1, y_test1), shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HsNhn09NhXR"
      },
      "source": [
        "### 1.5. 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB1DGNnlNZUV"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9desSnK0NmDx"
      },
      "outputs": [],
      "source": [
        "epochs = range(1,len(acc)+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZCdmA9zN9z5"
      },
      "source": [
        "정확도와 손실함수 그래프 그리기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nksI66aLNo_d"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc, 'b', color='blue', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', color='red', label='Validation acc')\n",
        "plt.title('Training and validation accuracy', color='w')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', color='blue', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
        "plt.title('Training and validation loss', color='w')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_KMswpkB-yT"
      },
      "source": [
        "학습 결과 추론하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFLW5MRmNtcm"
      },
      "outputs": [],
      "source": [
        "test1 = x_test1[0]\n",
        "print(test1.shape)\n",
        "plt.imshow(test1[...,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-77kUNwwCGEF"
      },
      "outputs": [],
      "source": [
        "testp = x_test1[:100]\n",
        "testg = y_test[:100]\n",
        "scores = model.predict(testp)\n",
        "\n",
        "new_scores = []\n",
        "for score in scores:\n",
        "  max_val = np.max(score)\n",
        "  prob_num = list(score).index(max_val)\n",
        "  new_scores.append(prob_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovj6NjIwCKV7"
      },
      "outputs": [],
      "source": [
        "plt.imshow(testp[0,...,0])\n",
        "print(f'label={testg[0]}, predict={new_scores[0]}')\n",
        "print(scores[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hLPRD53-k6-"
      },
      "source": [
        "## 2. VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8oMRw4jCvug"
      },
      "source": [
        "### 2.1. Data Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSHZ7fLECS8D"
      },
      "outputs": [],
      "source": [
        "from skimage import morphology\n",
        "from skimage import measure\n",
        "from skimage import exposure\n",
        "from skimage import transform as skit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd7R5lv0CFmG"
      },
      "outputs": [],
      "source": [
        "x_train_list = []\n",
        "x_test_list = []\n",
        "for i, i_ in enumerate(x_train[:1000]):\n",
        "    x_train_list.append(skit.resize(i_, (32, 32)))\n",
        "for i, i_ in enumerate(x_test[:500]):\n",
        "    x_test_list.append(skit.resize(i_, (32, 32)))\n",
        "\n",
        "x_train1 = np.expand_dims(np.array(x_train_list), axis=-1)\n",
        "x_test1 = np.expand_dims(np.array(x_test_list), axis=-1)\n",
        "print(x_train1.shape, x_test1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeK3ZmvECk_V"
      },
      "outputs": [],
      "source": [
        "y_train_list = []\n",
        "y_test_list = []\n",
        "for i, i_ in enumerate(y_train[:1000]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_train_list.append(zero)\n",
        "\n",
        "for i, i_ in enumerate(y_test[:500]):\n",
        "    zero = [0]*10\n",
        "    zero[i_] = 1\n",
        "    y_test_list.append(zero)    \n",
        "    \n",
        "y_train1 = np.array(y_train_list)\n",
        "y_test1 = np.array(y_test_list)\n",
        "print(y_train1.shape, y_test1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmggEJhEClUQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(x_train1[i][...,0], cmap='gray')\n",
        "    plt.title('Class = {}'.format(y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpIZIZzWCyqn"
      },
      "source": [
        "### 2.2. Model Build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHqT4dLEE4u"
      },
      "source": [
        "* 2.2.1. VGG code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMgL9poaCubZ"
      },
      "outputs": [],
      "source": [
        "def conv_block_2d(lr_conv, lr_num, par_list, bkn):\n",
        "        # parameter\n",
        "        filter_num = par_list[0]\n",
        "        conv_size = par_list[1]\n",
        "        conv_act = par_list[2]\n",
        "        pool_size = par_list[3]\n",
        "        # code\n",
        "        for i in range(lr_num):\n",
        "            lr_conv = layers.Conv2D(filter_num, conv_size, activation=None, padding='same', \n",
        "                                    kernel_initializer='he_normal',\n",
        "                                    name='block{}_conv{}'.format(bkn, i+1))(lr_conv)\n",
        "            lr_conv = layers.BatchNormalization(axis=-1, name='block{}_batchnorm{}'.format(bkn, i+1))(lr_conv)\n",
        "            lr_conv = layers.Activation(conv_act, name='block{}_activ{}'.format(bkn, i+1))(lr_conv)\n",
        "        lr_pool = layers.MaxPooling2D(pool_size=pool_size, name='block{}_pool'.format(bkn, i+1))(lr_conv)\n",
        "        return lr_pool\n",
        "\n",
        "def output_block(lr_dense, block_num, dens_count, act_func, drop_rate):\n",
        "    lr_dense = layers.Flatten(name='flatten_layer')(lr_dense)\n",
        "    for i in range(block_num):\n",
        "        lr_dense = layers.Dense(dens_count[i], kernel_regularizer=None,\n",
        "                                activation=act_func, name='classifier_dense_{}'.format(i+1))(lr_dense)\n",
        "        lr_dense = layers.Dropout(drop_rate, name='classifier_dropout_{}'.format(i+1))(lr_dense)\n",
        "    return lr_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suOXhB3iDS8g"
      },
      "outputs": [],
      "source": [
        "def VGG16_2D(par_dic):\n",
        "    # parameters\n",
        "    input_size = par_dic['input_size']\n",
        "    conv_size = par_dic['conv_size']\n",
        "    conv_act = par_dic['conv_act']\n",
        "    pool_size = par_dic['pool_size']\n",
        "    dens_num = par_dic['dens_num']\n",
        "    dens_count = par_dic['dens_count']\n",
        "    dens_act = par_dic['dens_act']\n",
        "    drop_out = par_dic['drop_out']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "\n",
        "    # code block\n",
        "    inputs = Input(shape=(input_size, input_size, 1), name='input_layer')\n",
        "    block1 = conv_block_2d(inputs, 2, [64, conv_size, conv_act, pool_size], 1)\n",
        "    block2 = conv_block_2d(block1, 2, [128, conv_size, conv_act, pool_size], 2)\n",
        "    block3 = conv_block_2d(block2, 3, [256, conv_size, conv_act, pool_size], 3)\n",
        "    block4 = conv_block_2d(block3, 3, [512, conv_size, conv_act, pool_size], 4)\n",
        "    block5 = conv_block_2d(block4, 3, [512, conv_size, conv_act, pool_size], 5)\n",
        "    dens = output_block(block5, dens_num, dens_count, dens_act, drop_out)\n",
        "    outputs = layers.Dense(output_count, activation=output_act, name='output_layer')(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYHQhdfwDCH5"
      },
      "outputs": [],
      "source": [
        "def VGG19_2D(par_dic):\n",
        "    # parameters\n",
        "    input_size = par_dic['input_size']\n",
        "    conv_size = par_dic['conv_size']\n",
        "    conv_act = par_dic['conv_act']\n",
        "    pool_size = par_dic['pool_size']\n",
        "    dens_num = par_dic['dens_num']\n",
        "    dens_count = par_dic['dens_count']\n",
        "    dens_act = par_dic['dens_act']\n",
        "    drop_out = par_dic['drop_out']\n",
        "    output_count = par_dic['output_count']\n",
        "    output_act = par_dic['output_act']\n",
        "\n",
        "    # code block\n",
        "    inputs = Input(shape=(input_size, input_size, 1))\n",
        "    block1 = conv_block_2d(inputs, 2, [64, conv_size, conv_act, pool_size], 1)\n",
        "    block2 = conv_block_2d(block1, 2, [128, conv_size, conv_act, pool_size], 2)\n",
        "    block3 = conv_block_2d(block2, 4, [256, conv_size, conv_act, pool_size], 3)\n",
        "    block4 = conv_block_2d(block3, 4, [512, conv_size, conv_act, pool_size], 4)\n",
        "    block5 = conv_block_2d(block4, 4, [512, conv_size, conv_act, pool_size], 5)\n",
        "    dens = output_block(block5, dens_num, dens_count, dens_act, drop_out)\n",
        "    outputs = layers.Dense(output_count, activation=output_act)(dens)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJrsVXFELCF"
      },
      "source": [
        "* 2.2.2 VGG Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBH0JyXGDSMg"
      },
      "outputs": [],
      "source": [
        "network_param_2d = {'input_size': 32,\n",
        "                     'conv_size': 3,\n",
        "                     'conv_act': 'relu',\n",
        "                     'pool_size': 2,\n",
        "                     'dens_num': 2,\n",
        "                     'dens_count': [1000,500],\n",
        "                     'dens_act': 'relu',\n",
        "                     'drop_out': 0.5,\n",
        "                     'output_count': 10,\n",
        "                     'output_act': 'softmax'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqHmqfgTD-Oz"
      },
      "outputs": [],
      "source": [
        "model = VGG16_2D(network_param_2d)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WsHElLPEBfO"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=optimizers.Adam(lr=1e-4), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKhnowo1EZtx"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train1, y_train1, epochs=20, batch_size=32, \n",
        "                    validation_data=(x_test1, y_test1), shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tet0PzxmFU_U"
      },
      "source": [
        "### 2.3. Model Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vfxwSafEsu_"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "# Accuracy graph\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(epochs, acc, 'b', label='Training acc = {}%'.format(np.around(np.max(acc) * 100, decimals=1)))\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc = {}%'.format(np.around(np.max(val_acc) * 100, decimals=1)))\n",
        "plt.title('{} Accuracy (Total Epoch = {})'.format('VGG16', len(acc)), fontsize=15, y=1.02)\n",
        "plt.xticks(size=15)\n",
        "plt.yticks(size=15)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()\n",
        "# Loss graph\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(epochs, loss, 'b', label='Training loss = {}'.format(np.around(np.min(loss), decimals=3)))\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss= {}'.format(np.around(np.min(val_loss), decimals=3)))\n",
        "plt.title('{} Loss (Total Epoch = {})'.format('VGG16', len(loss)), fontsize=15, y=1.02)\n",
        "plt.xticks(size=15)\n",
        "plt.yticks(size=15)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsKsB1J5FQIW"
      },
      "source": [
        "### 2.4. Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iURQXorTE3Nd"
      },
      "outputs": [],
      "source": [
        "test1 = x_test1[0]\n",
        "print(test1.shape)\n",
        "plt.imshow(test1[...,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPRNaZHAE5Ja"
      },
      "outputs": [],
      "source": [
        "testp = x_test1[:100]\n",
        "testg = y_test[:100]\n",
        "scores = model.predict(testp)\n",
        "\n",
        "new_scores = []\n",
        "for score in scores:\n",
        "  max_val = np.max(score)\n",
        "  prob_num = list(score).index(max_val)\n",
        "  new_scores.append(prob_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LD2oneiE53X"
      },
      "outputs": [],
      "source": [
        "plt.imshow(testp[0,...,0])\n",
        "print(f'label={testg[0]}, predict={new_scores[0]}')\n",
        "print(scores[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv2nkq6lFl5w"
      },
      "source": [
        "## 3. Segmentation: FCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fz8CRsiHnak"
      },
      "source": [
        "https://github.com/Pulsar-kkaturi/DL-Education/blob/master/Notebooks/CNN_FCN_Build.ipynb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Lecture1_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

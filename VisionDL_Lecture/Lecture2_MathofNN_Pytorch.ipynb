{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2. 신경망의 수학적 이해\n",
    "* Ref. 최건호, 파이토치 첫걸음, 한빛미디어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 21:02:11.322064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 21:02:12.047239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Framework 비교\n",
    "* 선형 연산 (linear operation): y = W * x + z (W: kernel, z: bias)\n",
    "* 속도 비교: Numpy vs Tensorflow vs Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient x =  [[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]]\n",
      "gradient W =  [[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n",
      "gradient z =  [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "# Operation Time =  0:00:00.001710\n"
     ]
    }
   ],
   "source": [
    "# 연산에 필요한 numpy, 시간을 측정하기 위해 datetime을 불러옵니다.\n",
    "start = datetime.now()\n",
    "\n",
    "# 랜덤하게 3x4 형태의 변수 x,y,z를 설정해줍니다.\n",
    "np.random.seed(0)\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x = np.random.randn(N,D)\n",
    "w = np.random.randn(N,D)\n",
    "z = np.random.randn(N,D)\n",
    "\n",
    "# x,y,z를 이용해 x*y+z를 계산해줍니다.\n",
    "a = x * w\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "# 기울기(gradient)가 1이라고 가정하고 역전파를 해줍니다. 역전파에 대한 내용은 4장에서 자세히 다룹니다.\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_w = grad_a * w\n",
    "grad_x = grad_a * x\n",
    "\n",
    "# 각각의 기울기가 몇인지 걸린 시간은 얼마인지 확인해봅니다.\n",
    "print('gradient x = ', grad_x)\n",
    "print('gradient W = ', grad_w)\n",
    "print('gradient z = ', grad_z)\n",
    "print('# Operation Time = ', datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient x =  tf.Tensor(\n",
      "[[0.5554141  0.22129297 0.8649249  0.77728355]\n",
      " [0.6451167  0.53036225 0.01444101 0.87350917]\n",
      " [0.4697218  0.38672888 0.88615394 0.6367892 ]], shape=(3, 4), dtype=float32)\n",
      "gradient W =  tf.Tensor(\n",
      "[[0.29197514 0.20656645 0.53539073 0.5612575 ]\n",
      " [0.4166745  0.80782795 0.4932251  0.99812925]\n",
      " [0.69673514 0.1253736  0.7098167  0.6624156 ]], shape=(3, 4), dtype=float32)\n",
      "gradient z =  tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n",
      "# Operation Time =  0:00:05.257465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 21:02:23.034628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11122 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:05:00.0, compute capability: 5.2\n",
      "2023-04-12 21:02:23.035312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 11133 MB memory:  -> device: 1, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:06:00.0, compute capability: 5.2\n",
      "2023-04-12 21:02:23.035830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 11133 MB memory:  -> device: 2, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:09:00.0, compute capability: 5.2\n",
      "2023-04-12 21:02:23.036361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 11133 MB memory:  -> device: 3, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:0a:00.0, compute capability: 5.2\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 텐서플로 프레임워크를 이용해 같은 연산을 해보도록 하겠습니다.\n",
    "start = datetime.now()\n",
    "gpu_num = 0 # 일반적으로 0번이지만, 자신의 환경에 맞게 설정\n",
    "\n",
    "# 텐서플로는 연산 그래프를 먼저 정의하고 추후에 여기에 값을 전달하는 방식입니다. 여기서는 비어있는 그래프만 정의해줍니다.\n",
    "# Define Graph on GPU\n",
    "with tf.device(f'/gpu:{gpu_num}'):              # 해당 연산을 위에서 지정한 gpu에서 하겠다는 의미입니다.\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    N,D = 3,4\n",
    "    \n",
    "    x = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
    "    w = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
    "    z = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:        # 텐서플로의 경우 계산 그래프에서 작동합니다.\n",
    "        a = x * w                          # 연산 과정 또한 정의해줍니다.\n",
    "        b = a + z\n",
    "        c = tf.reduce_sum(b)\n",
    "        [grad_x, grad_w, grad_z] = tape.gradient(c,[x,w,z])  # c에 대한 x,y,z의 기울기(gradient)를 구하고 이를 각각 grad_x, grad_y, grad_z에 저장하도록 지정해놓습니다.\n",
    "\n",
    "# 값들을 확인하고 걸린 시간을 측정합니다.\n",
    "print('gradient x = ', grad_x)\n",
    "print('gradient W = ', grad_w)\n",
    "print('gradient z = ', grad_z)\n",
    "print('# Operation Time = ', datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient x =  tf.Tensor(\n",
      "[[0.5554141  0.22129297 0.8649249  0.77728355]\n",
      " [0.6451167  0.53036225 0.01444101 0.87350917]\n",
      " [0.4697218  0.38672888 0.88615394 0.6367892 ]], shape=(3, 4), dtype=float32)\n",
      "gradient W =  tf.Tensor(\n",
      "[[0.29197514 0.20656645 0.53539073 0.5612575 ]\n",
      " [0.4166745  0.80782795 0.4932251  0.99812925]\n",
      " [0.69673514 0.1253736  0.7098167  0.6624156 ]], shape=(3, 4), dtype=float32)\n",
      "gradient z =  tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n",
      "# Operation Time =  0:00:00.055343\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 파이토치를 이용해 같은 연산을 진행해보도록 하겠습니다.\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "gpu_num = 0 # 일반적으로 0번이지만, 자신의 환경에 맞게 설정\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "# x,y,z를 랜덤하게 초기화 해줍니다. \n",
    "# https://pytorch.org/docs/stable/torch.html?highlight=randn#torch.randn\n",
    "\n",
    "x = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
    "w = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
    "z = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
    "\n",
    "# 연산 그래프는 정의됨과 동시에 연산됩니다.\n",
    "a = x * w\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "# 역전파 실행 (Numpy에서는 해당 과정을 직접 풀어서 작성하였다)\n",
    "c.backward()\n",
    "\n",
    "\n",
    "# 각각의 기울기와 걸린 시간을 출력합니다.\n",
    "print('gradient x = ', grad_x)\n",
    "print('gradient W = ', grad_w)\n",
    "print('gradient z = ', grad_z)\n",
    "print('# Operation Time = ', datetime.now()-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텐서 연산\n",
    "* 텐서 생성 (Tensor Creation)\n",
    "* 기울기 계산 (Gradient Operation)\n",
    "* 인퍼런스 (Inference)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 토치 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3104e+27,  4.5588e-41, -1.3104e+27],\n",
      "        [ 4.5588e-41,  5.0000e+00,  6.0000e+00]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_rand = torch.Tensor(2,3) # 난수 생성\n",
    "x_data = torch.Tensor([[1,2,3],[4,5,6]]) # 데이터 지정\n",
    "print(x_rand)\n",
    "print(x_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Torch Tensor 사용법\n",
    "    *   Ref link (파이토치 홈페이지): https://pytorch.org/docs/stable/tensors.html?highlight=torch+tensor#torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], device='cuda:0', requires_grad=True)\n",
      "tensor([1., 2., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data_arr = [[1,2,3],[4,5,6]]\n",
    "x_tensor = torch.tensor(data=data_arr,   # 텐서로 변환시킬 데이터 행렬\n",
    "                        dtype=torch.float32,    # 토치 타입\n",
    "                        device=torch.device('cuda:0'),    # 저장할 디바이스 (CPU vs GPU)\n",
    "                        requires_grad=True)     # 계산한 기울기 저장 여부\n",
    "\n",
    "# GPU 텐서를 바로 생성할 수도 있다 (Data type도 float으로 미리 설정).\n",
    "y_tensor = torch.cuda.FloatTensor([1,2,3]) \n",
    "print(x_tensor)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 가능한 GPU가 존재하는가? (True or False):  True\n",
      "사용 가능한 GPU의 수는 4 개 입니다.\n",
      "GPU 각각의 이름은 아래와 같습니다.\n",
      "GPU 0: NVIDIA GeForce GTX TITAN X\n",
      "GPU 1: NVIDIA GeForce GTX TITAN X\n",
      "GPU 2: NVIDIA GeForce GTX TITAN X\n",
      "GPU 3: NVIDIA GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "# 파이토치에서 GPU 사용은 핵심이다. 아래의 코드로 사용가능한 GPU를 확인가능하다!\n",
    "import os, torch\n",
    "\n",
    "print(\"사용 가능한 GPU가 존재하는가? (True or False): \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 가능한 GPU의 수는 {} 개 입니다.\".format(torch.cuda.device_count()))\n",
    "    print(\"GPU 각각의 이름은 아래와 같습니다.\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "else:\n",
    "    print(\"사용 가능한 GPU가 존재하지 않습니다. 혹은 GPU를 Pytorch가 찾지 못하고 있습니다.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 기울기 계산"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* requires_grad를 True로 활성화 시킬 경우 기울기 계산이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([2., 3., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "x = torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "y = torch.tensor([2.,3.,4.],requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# 기울기 계산\n",
    "z = x + y\n",
    "z.sum().backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 간단한 연산의 역전파 (z = 2x<sup>2</sup> + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* gradient =  tensor([ 8., 12.]) None None\n",
      "* 손실값 =  tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30111/409920484.py:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  print('* gradient = ', x.grad, y.grad, z.grad)\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성 및 선형 연산\n",
    "x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n",
    "y = x**2\n",
    "z = 2*y +3\n",
    "\n",
    "# 목표값을 지정합니다. \n",
    "target = torch.tensor([3.0,4.0])\n",
    "\n",
    "# z와 목표값의 절대값 차이를 계산합니다. \n",
    "# backward는 스칼라 값에 대해서 동작하기 때문에 길이 2짜리 텐서인 loss를 torch.sum을 통해 하나의 숫자로 바꿔줍니다.\n",
    "loss = torch.sum(torch.abs(z-target))\n",
    "\n",
    "# 그리고 스칼라 값이 된 loss에 대해 backward를 적용합니다.\n",
    "loss.backward()\n",
    "\n",
    "# 여기서 y와 z는 기울기가 None으로 나오는데 이는 x,y,z중에 x만이 leaf node이기 때문입니다.\n",
    "print('* gradient = ', x.grad, y.grad, z.grad)\n",
    "print('* 손실값 = ', loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기울기 계산이 완료되었다면(즉 학습 완료!), 기울기 계산을 끄고 결과값만 추론할 수 있다.\n",
    "* 학습완료된 모델을 사용하여, 새로운 예측값을 결과로 얻어내는 과정을 'inference'라고 부른다!\n",
    "* no_grad()를 사용하면 기울기 계산을 끌 수 있다. (사실 신경망 모델에서는 model.eval()을 쓰면 된다! Lecture 4에서 배울 것이다!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad,y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x + y\n",
    "    print(z.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 선형 회귀 분석 (linear regression analysis)\n",
    "* 선형 모델 (Linear Model)\n",
    "* y = 2x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

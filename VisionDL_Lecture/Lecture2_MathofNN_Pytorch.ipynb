{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pulsar-kkaturi/DL-Education/blob/master/VisionDL_Lecture/Lecture2_MathofNN_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtnlffl0EwiH"
      },
      "source": [
        "# Lecture 2. 신경망의 수학적 이해\n",
        "* Ref. 최건호, 파이토치 첫걸음, 한빛미디어 ([link](https://drive.google.com/drive/folders/12zphz36T6gEJac6WScnvRN27-f1tfHO1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vrN2MiEwiK"
      },
      "source": [
        "## 0. Framework 비교\n",
        "* 선형 연산 (linear operation): y = W * x + z (W: kernel, z: bias)\n",
        "* 속도 비교: Numpy vs Tensorflow vs Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGG05FJXEwiM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPXE2eSeEwiO"
      },
      "outputs": [],
      "source": [
        "# 연산에 필요한 numpy, 시간을 측정하기 위해 datetime을 불러옵니다.\n",
        "start = datetime.now()\n",
        "\n",
        "# 랜덤하게 3x4 형태의 변수 x,y,z를 설정해줍니다.\n",
        "np.random.seed(0)\n",
        "\n",
        "N,D = 3,4\n",
        "\n",
        "x = np.random.randn(N,D)\n",
        "w = np.random.randn(N,D)\n",
        "z = np.random.randn(N,D)\n",
        "\n",
        "# x,y,z를 이용해 x*y+z를 계산해줍니다.\n",
        "a = x * w\n",
        "b = a + z\n",
        "c = np.sum(b)\n",
        "\n",
        "# 기울기(gradient)가 1이라고 가정하고 역전파를 해줍니다. 역전파에 대한 내용은 4장에서 자세히 다룹니다.\n",
        "grad_c = 1.0\n",
        "grad_b = grad_c * np.ones((N,D))\n",
        "grad_a = grad_b.copy()\n",
        "grad_z = grad_b.copy()\n",
        "grad_w = grad_a * w\n",
        "grad_x = grad_a * x\n",
        "\n",
        "# 각각의 기울기가 몇인지 걸린 시간은 얼마인지 확인해봅니다.\n",
        "print('gradient x = ', grad_x)\n",
        "print('gradient W = ', grad_w)\n",
        "print('gradient z = ', grad_z)\n",
        "print('# Operation Time = ', datetime.now()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RAMy8KjEwiP"
      },
      "outputs": [],
      "source": [
        "# 이번에는 텐서플로 프레임워크를 이용해 같은 연산을 해보도록 하겠습니다.\n",
        "start = datetime.now()\n",
        "gpu_num = 0 # 일반적으로 0번이지만, 자신의 환경에 맞게 설정\n",
        "\n",
        "# 텐서플로는 연산 그래프를 먼저 정의하고 추후에 여기에 값을 전달하는 방식입니다. 여기서는 비어있는 그래프만 정의해줍니다.\n",
        "# Define Graph on GPU\n",
        "with tf.device(f'/gpu:{gpu_num}'):              # 해당 연산을 위에서 지정한 gpu에서 하겠다는 의미입니다.\n",
        "    tf.random.set_seed(0)\n",
        "\n",
        "    N,D = 3,4\n",
        "\n",
        "    x = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
        "    w = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
        "    z = tf.Variable(tf.random.uniform(shape=(N,D)), dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:        # 텐서플로의 경우 계산 그래프에서 작동합니다.\n",
        "        a = x * w                          # 연산 과정 또한 정의해줍니다.\n",
        "        b = a + z\n",
        "        c = tf.reduce_sum(b)\n",
        "        [grad_x, grad_w, grad_z] = tape.gradient(c,[x,w,z])  # c에 대한 x,y,z의 기울기(gradient)를 구하고 이를 각각 grad_x, grad_y, grad_z에 저장하도록 지정해놓습니다.\n",
        "\n",
        "# 값들을 확인하고 걸린 시간을 측정합니다.\n",
        "print('gradient x = ', grad_x)\n",
        "print('gradient W = ', grad_w)\n",
        "print('gradient z = ', grad_z)\n",
        "print('# Operation Time = ', datetime.now()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj3I0lpLEwiQ"
      },
      "outputs": [],
      "source": [
        "# 이번에는 파이토치를 이용해 같은 연산을 진행해보도록 하겠습니다.\n",
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "gpu_num = 0 # 일반적으로 0번이지만, 자신의 환경에 맞게 설정\n",
        "\n",
        "N,D = 3,4\n",
        "\n",
        "# x,y,z를 랜덤하게 초기화 해줍니다.\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=randn#torch.randn\n",
        "\n",
        "x = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
        "w = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
        "z = torch.randn(N,D,device=torch.device(f'cuda:{gpu_num}'), requires_grad=True)\n",
        "\n",
        "# 연산 그래프는 정의됨과 동시에 연산됩니다.\n",
        "a = x * w\n",
        "b = a + z\n",
        "c = torch.sum(b)\n",
        "\n",
        "# 역전파 실행 (Numpy에서는 해당 과정을 직접 풀어서 작성하였다)\n",
        "c.backward()\n",
        "\n",
        "\n",
        "# 각각의 기울기와 걸린 시간을 출력합니다.\n",
        "print('gradient x = ', x.grad)\n",
        "print('gradient W = ', w.grad)\n",
        "print('gradient z = ', z.grad)\n",
        "print('# Operation Time = ', datetime.now()-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQfsqxyzEwiR"
      },
      "source": [
        "## 1. 텐서 연산\n",
        "* 텐서 생성 (Tensor Creation)\n",
        "* 기울기 계산 (Gradient Operation)\n",
        "* 인퍼런스 (Inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr-p0-KGEwiR"
      },
      "source": [
        "### 1.1. 토치 텐서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r51YWnEREwiR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x_rand = torch.Tensor(2,3) # 난수 생성\n",
        "x_data = torch.Tensor([[1,2,3],[4,5,6]]) # 데이터 지정\n",
        "print(x_rand)\n",
        "print(x_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klodv8TxEwiS"
      },
      "source": [
        "* Torch Tensor 사용법\n",
        "    *   Ref link (파이토치 홈페이지): https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeMlu9UnEwiS"
      },
      "outputs": [],
      "source": [
        "data_arr = [[1,2,3],[4,5,6]]\n",
        "x_tensor = torch.tensor(data=data_arr,   # 텐서로 변환시킬 데이터 행렬\n",
        "                        dtype=torch.float32,    # 토치 타입\n",
        "                        device=torch.device('cuda:0'),    # 저장할 디바이스 (CPU vs GPU)\n",
        "                        requires_grad=True)     # 계산한 기울기 저장 여부\n",
        "\n",
        "# GPU 텐서를 바로 생성할 수도 있다 (Data type도 float으로 미리 설정).\n",
        "y_tensor = torch.cuda.FloatTensor([1,2,3])\n",
        "print(x_tensor)\n",
        "print(y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGkkkWetEwiS"
      },
      "outputs": [],
      "source": [
        "# 파이토치에서 GPU 사용은 핵심이다. 아래의 코드로 사용가능한 GPU를 확인가능하다!\n",
        "import os, torch\n",
        "\n",
        "print(\"사용 가능한 GPU가 존재하는가? (True or False): \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"사용 가능한 GPU의 수는 {} 개 입니다.\".format(torch.cuda.device_count()))\n",
        "    print(\"GPU 각각의 이름은 아래와 같습니다.\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
        "else:\n",
        "    print(\"사용 가능한 GPU가 존재하지 않습니다. 혹은 GPU를 Pytorch가 찾지 못하고 있습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np0JP29cEwiS"
      },
      "source": [
        "### 1.2. 기울기 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nzIVaX3EwiS"
      },
      "source": [
        "* requires_grad를 True로 활성화 시킬 경우 기울기 계산이 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um4rDuXlEwiT"
      },
      "outputs": [],
      "source": [
        "# 텐서 생성\n",
        "x = torch.tensor([1.,2.,3.],requires_grad=True)\n",
        "y = torch.tensor([2.,3.,4.],requires_grad=True)\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "# 기울기 계산\n",
        "z = x + y\n",
        "z.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyLWI3QuEwiT"
      },
      "source": [
        "* 간단한 연산의 역전파 (z = 2x<sup>2</sup> + 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7rTdlLSEwiT"
      },
      "outputs": [],
      "source": [
        "# 텐서 생성 및 선형 연산\n",
        "x = torch.tensor(data=[2.0,3.0],requires_grad=True)\n",
        "y = x**2\n",
        "z = 2*y +3\n",
        "\n",
        "# 목표값을 지정합니다.\n",
        "target = torch.tensor([3.0,4.0])\n",
        "\n",
        "# z와 목표값의 절대값 차이를 계산합니다.\n",
        "# backward는 스칼라 값에 대해서 동작하기 때문에 길이 2짜리 텐서인 loss를 torch.sum을 통해 하나의 숫자로 바꿔줍니다.\n",
        "loss = torch.sum(torch.abs(z-target))\n",
        "\n",
        "# 그리고 스칼라 값이 된 loss에 대해 backward를 적용합니다.\n",
        "loss.backward()\n",
        "\n",
        "# 여기서 y와 z는 기울기가 None으로 나오는데 이는 x,y,z중에 x만이 leaf node이기 때문입니다.\n",
        "print('* gradient = ', x.grad, y.grad, z.grad)\n",
        "print('* 손실값 = ', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q8FESiJEwiT"
      },
      "source": [
        "* 기울기 계산이 완료되었다면(즉 학습 완료!), 기울기 계산을 끄고 결과값만 추론할 수 있다.\n",
        "* 학습완료된 모델을 사용하여, 새로운 예측값을 결과로 얻어내는 과정을 'inference'라고 부른다!\n",
        "* no_grad()를 사용하면 기울기 계산을 끌 수 있다. (사실 신경망 모델에서는 model.eval()을 쓰면 된다! Lecture 4에서 배울 것이다!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xMwDDGoEwiT"
      },
      "outputs": [],
      "source": [
        "print(x.requires_grad,y.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = x + y\n",
        "    print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH27-QFQEwiT"
      },
      "source": [
        "## 2. 선형 회귀 분석 (linear regression analysis)\n",
        "* 선형 모델 (Linear Model) : y = xA<sup>T</sup>+b (* 파이토치 API 참고) (https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
        "* y = 2x + 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E_7oThDEwiU"
      },
      "source": [
        "### 2.1. 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_vl47xtEwiU"
      },
      "outputs": [],
      "source": [
        "# 넘파이와 파이토치를 불러옵니다.\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Neural Network의 약자로 인공신경망 연산들이 들어가 있습니다. (ex. Linear, Convolution, RNN 등등)\n",
        "import torch.nn as nn\n",
        "\n",
        "# 모델을 최적화 하는데 필요한 최적화 함수들이 들어가 있습니다 (ex. SGD, ADAM, LBFGS 등등)\n",
        "import torch.optim as optim\n",
        "\n",
        "# 텐서를 초기화 하는 함수들이 들어가 있습니다. (ex. uniform, normal, xavier 등등)\n",
        "import torch.nn.init as init\n",
        "\n",
        "# 데이터나 학습 결과를 출력하기 위해 사용합니다.\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDlPDoGmEwiU"
      },
      "source": [
        "### 2.2. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-fbQb67EwiU"
      },
      "outputs": [],
      "source": [
        "# 데이터의 개수는 1000개, 학습 횟수는 500회로 지정해줍니다.\n",
        "# 이는 임의로 지정한 수치입니다.\n",
        "\n",
        "num_data = 1000\n",
        "num_epoch = 500\n",
        "\n",
        "# 데이터에 추가할 노이즈를 정규분포를 따르게 만들어줍니다.\n",
        "# 이때 평균은 디폴트로 0, 편차는 0.2로 임의로 지정했습니다.\n",
        "noise = init.normal_(torch.FloatTensor(num_data,1),std=0.5)\n",
        "\n",
        "# x 는 -10에서 10에서 uniform 하게 생성합니다.\n",
        "# 이렇게 되면 x는 1000x1 형태를 가지고 -10에서 10 사이의 값들을 uniform 하게 갖게 됩니다.\n",
        "x = init.uniform_(torch.Tensor(num_data,1),-10,10)\n",
        "\n",
        "# 연산 그래프를 정의합니다.\n",
        "y = 2*x+3\n",
        "\n",
        "# y에 노이즈를 더해 y_noise를 만들어줍니다.\n",
        "# 학습때 y_noise를 목표값으로 사용합니다.\n",
        "# 이렇게 하는 이유는 실제 데이터를 사용할 때 여러 측정과정에서 노이즈가 추가되는 경우가 많기 때문입니다.\n",
        "y_noise = y+noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6QP7WQ5EwiU"
      },
      "outputs": [],
      "source": [
        "# matplotlib의 scatter 함수를 사용해 학습 데이터를 확인합니다.\n",
        "\n",
        "# figure의 크기를 지정해줍니다.\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "# x축에는 x를 사용하고 y축에는 y_noise를 사용해 scatter plot 해줍니다.\n",
        "# 이때 점의 크기는 7, 점의 색상은 회색으로 임의로 지정했습니다.\n",
        "# 실제 정답인 y=2x+3의 값(noise가 추가 되지않은 y)은 파란색으로 표시\n",
        "plt.scatter(x.numpy(),y_noise.numpy(),s=5,c=\"gray\")\n",
        "plt.scatter(x.numpy(),y.numpy(),s=5,c=\"blue\")\n",
        "\n",
        "# figure의 x,y 축 범위를 지정해줍니다.\n",
        "plt.axis([-12, 12, -25, 25])\n",
        "\n",
        "# figure를 출력합니다.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TiQELrKEwiU"
      },
      "source": [
        "### 2.3. Model 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SpkA4cKEwiU"
      },
      "outputs": [],
      "source": [
        "# 선형 모델을 생성합니다.\n",
        "# 입력으로 들어오는 x가 1000x1 의 형태를 가지고 있고 여기서 특성의 개수는 1개이기 때문에 앞에 1이 들어가게 됩니다. Linear(1,?)\n",
        "# 출력으로 기대하는 값 또한 1000x1 의 형태이기 때문에 특성의 개수가 1개. 그렇기 때문에 뒤에 1이 들어갑니다.      Linear(?,1)\n",
        "model = nn.Linear(1,1)\n",
        "\n",
        "# 손실 함수를 지정해줍니다.\n",
        "# 임의로 L1 손실을 사용했습니다.\n",
        "loss_func = nn.L1Loss()\n",
        "\n",
        "# 최적화 함수를 지정해줍니다.\n",
        "# 이때 인수로 학습의 대상이 되는 모델의 변수(model.parameters())를 전달합니다.\n",
        "# 또한 학습률은 0.01로 임의로 지정했습니다.\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7iodWtCEwiV"
      },
      "source": [
        "### 2.4. Model 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlSw0tqwEwiV"
      },
      "outputs": [],
      "source": [
        "# 손실이 어떻게 변하는지 확인하기 위해 loss_arr를 만들어 기록합니다.\n",
        "loss_arr =[]\n",
        "\n",
        "# 또한 목표값은 y_noise로 지정해줍니다.\n",
        "label = y_noise\n",
        "\n",
        "# 500으로 지정했던 학습 횟수만큼 반복합니다.\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # 이전 학습의 기울기를 지우고 최적화 함수를 초기화해줍니다.\n",
        "    # 기울기를 지우지 않으면 기존의 업데이트 때문에 학습이 잘 이루어지지 않습니다.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 입력값 x를 모델에 넣어 결과값을 얻습니다.\n",
        "    output = model(x)\n",
        "\n",
        "    # 결과값과 목표값의 차이를 L1 손실 함수로 구해줍니다.\n",
        "    loss = loss_func(output,label)\n",
        "\n",
        "    # 손실에 대한 기울기를 구합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 구한 기울기를 이용해 모델의 변수를 업데이트 합니다.\n",
        "    optimizer.step()\n",
        "\n",
        "    # 50번 마다 모델의 변수가 어떻게 변하고 있는지 출력해줍니다.\n",
        "    if i % 50 == 0:\n",
        "        # https://pytorch.org/docs/stable/tensors.html?highlight=detach#torch.Tensor.detach\n",
        "        # 현재 연산 그래프에 속해있는 x, output 값을 detach를 통해 분리하고, 텐서를 넘파이 배열로 바꿔서 plt.scatter에 전달합니다.\n",
        "        plt.title(loss.data)\n",
        "        plt.scatter(x.detach().numpy(),output.detach().numpy())\n",
        "        plt.axis([-10, 10, -30, 30])\n",
        "        plt.show()\n",
        "\n",
        "    # 손실을 loss_arr에 추가해줍니다.\n",
        "    loss_arr.append(loss.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSTQ_QanEwiV"
      },
      "source": [
        "### 2.5. 학습 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEfGD7H2EwiV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(x.numpy(),y_noise.numpy(),s=5,c=\"gray\")\n",
        "plt.scatter(x.detach().numpy(),output.detach().numpy(),s=5,c=\"red\")\n",
        "plt.axis([-10, 10, -30, 30])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TqGsbI4EwiV"
      },
      "outputs": [],
      "source": [
        "# matplotlib의 plot 함수를 이용해 손실이 어떻게 줄어가는지 확인합니다.\n",
        "plt.plot(loss_arr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-trbgzT8EwiV"
      },
      "outputs": [],
      "source": [
        "# 현재 모델은 weight와 bias을 변수로 가지고 있는데 그 값들이 학습 후 실제 몇인지 수치적으로 확인해봅니다.\n",
        "param_list = list(model.parameters())\n",
        "print(\"Weight:\",param_list[0].item(),\"\\nBias:  \",param_list[1].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKA-eCyYEwiW"
      },
      "source": [
        "## # Pytorch의 핵심 Flow\n",
        "* Pytorch로 딥러닝 모델을 학습할 때, 가장 기본이 되는 코드 라인들\n",
        "* 단, 말 그대로 핵심 Flow를 보여주는 것이라 변수 지정이 안되어서 실행 시 에러 발생 유의!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSmDZE76EwiW"
      },
      "outputs": [],
      "source": [
        "# [1] 선형 모델 생성\n",
        "model = nn.Linear(1,1)\n",
        "\n",
        "# [2] 손실 함수(Loss Function)를 지정해줍니다. (L1 Loss)\n",
        "loss_func = nn.L1Loss()\n",
        "\n",
        "# [3] 최적화 함수(Optimizer)를 지정해줍니다.\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "# 학습 횟수(num_epoch)만큼 반복\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # 이전 학습의 기울기를 지우고 최적화 함수를 초기화해줍니다.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # [4] 입력값 x를 모델에 넣어 결과값을 얻습니다.\n",
        "    output = model(x)\n",
        "\n",
        "    # [5] 결과값과 목표값의 차이를 L1 손실 함수로 구해줍니다.\n",
        "    loss = loss_func(output,label)\n",
        "\n",
        "    # [6] 손실에 대한 기울기를 구합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # [7] 구한 기울기를 이용해 모델의 변수를 업데이트 합니다.\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i8jU-kCEwiW"
      },
      "source": [
        "## 3. 딥러닝을 활용한 비선형 회귀모델 학습 (Deep Neural Network; DNN)\n",
        "* 2장의 선형회귀모델과 달리, 비선형인 2차함수에 대한 회귀모델을 딥러닝으로 구현해보자!\n",
        "* y = x<sup>2</sup>+3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkgmxg0FEwiW"
      },
      "source": [
        "### 3.1. 라이브러리 불러오기 및 데이터 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVaAirUhEwiW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8haewdYEwiW"
      },
      "outputs": [],
      "source": [
        "# GPU에서 학습을 위해 GPU check\n",
        "print(\"사용 가능한 GPU가 존재하는가? (True or False): \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"사용 가능한 GPU의 수는 {} 개 입니다.\".format(torch.cuda.device_count()))\n",
        "    print(\"GPU 각각의 이름은 아래와 같습니다.\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
        "else:\n",
        "    print(\"사용 가능한 GPU가 존재하지 않습니다. 혹은 GPU를 Pytorch가 찾지 못하고 있습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWFVKafKEwij"
      },
      "outputs": [],
      "source": [
        "# 데이터 생성\n",
        "num_data = 1000\n",
        "\n",
        "noise = init.normal_(torch.FloatTensor(num_data,1),std=3)\n",
        "x = init.uniform_(torch.Tensor(num_data,1),-15,15)\n",
        "y = (x**2) + 3\n",
        "y_noise = y + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkLEVZYyEwij"
      },
      "outputs": [],
      "source": [
        "# 데이터 시각화\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(x.numpy(), y_noise.numpy(), s=3, c='gray', label='Original Data') # 학습시킬 실제 데이터 분포\n",
        "plt.scatter(x.numpy(), y.numpy(), s=2, c='red', label='Label Data') # 정답 분포\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxpeGuoLEwij"
      },
      "source": [
        "### 3.2. 모델 생성 및 학습\n",
        "* 3장의 예시처럼 하나의 값이 들어가서 하나의 값이 나오기 때문에 모델의 처음과 끝 특성의 개수는 1개입니다.\n",
        "\n",
        "* torch.nn.Sequential\n",
        "    * Sequential 모듈은 다양한 모듈을 담을 수 있는 일종의 리스트라고 보면 됩니다.\n",
        "    * Sequential 에 정의된 순서대로 연산이 진행되며, 많은 연산을 묶어서 한번에 관리할 수 있어서 편리합니다.\n",
        "* https://pytorch.org/docs/stable/nn.html#containers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXf5CvckEwij"
      },
      "outputs": [],
      "source": [
        "# 아래 코드는 특성의 개수가 1 -> 10 -> 20 -> 10 -> 1개로 변하는 인공신경망입니다.\n",
        "# 또한 선형변환 이후 활성화 함수를 넣어 비선형성이 생기도록 했습니다.\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(1,10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10,20),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(20,10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10,1),\n",
        "      )\n",
        "\n",
        "loss_func = nn.L1Loss() # 손실함수로는 L1(절대값의 평균) loss 사용\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.0002) # 옵티마이저로는 SGD 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8fN1kkMEwij"
      },
      "outputs": [],
      "source": [
        "num_epoch = 10000 # 학습시킬 epoch 수\n",
        "device = 'cuda:0' # 학습시킬 gpu\n",
        "loss_array = []\n",
        "pbar = tqdm(total=num_epoch) # tqdm으로 학습 진행도 확인 가능\n",
        "for i in range(num_epoch):\n",
        "    x.to(device)\n",
        "    y_noise.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "\n",
        "    loss = loss_func(output,y_noise)\n",
        "    loss.backward() # 역전파 손실 연산\n",
        "    optimizer.step() # 옵티마이저로 가중치 업데이트\n",
        "\n",
        "    pbar.set_description(f\"Processing({i+1}/{num_epoch}): loss={loss.data}\")\n",
        "    pbar.update(1)\n",
        "    loss_array.append(loss.detach().numpy()) # 손실값의 데이터만 numpy로 보냄\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8L6k7vEwik"
      },
      "source": [
        "### 3.3. 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPwmcsl2Ewik"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(loss_array)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsvE6PvhEwik"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(x.detach().numpy(),y_noise, s=3, c='grey', label=\"Original Data\")\n",
        "plt.scatter(x.detach().numpy(),y, s=3, c='red', label=\"Label Data\")\n",
        "plt.scatter(x.detach().numpy(),output.detach().numpy(), s=3, c='blue',label=\"Model Output\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lih4eq9iEwik"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
